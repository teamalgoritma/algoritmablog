<!DOCTYPE HTML>

<html>
    <head>
      
     <br>
      
      <div class="js-toggle-wrapper">
  <div class="js-toggle">
    <div class="js-toggle-track">
      <div class="js-toggle-track-check">
        <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAAlwSFlzAAALEwAACxMBAJqcGAAAAVlpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IlhNUCBDb3JlIDUuNC4wIj4KICAgPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4KICAgICAgPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIKICAgICAgICAgICAgeG1sbnM6dGlmZj0iaHR0cDovL25zLmFkb2JlLmNvbS90aWZmLzEuMC8iPgogICAgICAgICA8dGlmZjpPcmllbnRhdGlvbj4xPC90aWZmOk9yaWVudGF0aW9uPgogICAgICA8L3JkZjpEZXNjcmlwdGlvbj4KICAgPC9yZGY6UkRGPgo8L3g6eG1wbWV0YT4KTMInWQAABlJJREFUWAm1V3tsFEUcntnXvXu0tBWo1ZZHihBjCEWqkHiNaMLDRKOtQSKaiCFKQtS/SbxiFCHGCIkmkBSMwZhQNTFoQZD0DFiwtCDFAkdDqBBBKFj63rvdnfH7zfVo5aFBj0l2Z/dm5vd98/0es8dYjlpr62azufnDQNZcU1PciMfjWvb9rvZSMk4Ayfb36pLH13189GC8LAtIRLLPt+pzwrCuLq4ISEv/gHmitrAwfPbEkXc/ad4dL6iujrvyX0jcitgd/yZlZqftP6995Mr5TVLa22Tn8XVX2g/XLSRjUu7Q79jonS7I7hS7/0oOb5VyqF52n98oj7esXX07EjlxwXWisRmSnm3b29TTM8iYrjmFBWExubxwY/uhNas4r/WySl1fc5cetDMd7ydl+lMJJRw5WC8ud62Xx5rfepzwxgZmbhUYNS5Stvsj4yo2GXJEFBVHWDBkfdbR9HpYBaaUajDnBLKKpl1xRKYcgGtMCqEzTaSnThk/SQT0uJqTqFNBmXMCsZE48DzRZRMBRjv1GHNdk3HBImF9ZUvTyxM40pMKVc4JZBXQOLOFoDeKSxdp6HIQcO4rjYT9fn0pjbz9GLt7BAAODmjSVReXUMFzNW5x5vfxp2mIxZjIuQKJxAmFa+is2DQJJQ0JyBVExNOYcJnPxx/6/utnijmP555ALEagKAGGnGn64QORBjARcIA/yJk7JMJBLRrNtybTvH88KGjCf2jK86bhzmMcwDKFZEQvbIhxFYhChoMWMzU2iWznlIBEVJOsP+1bdX/ALx9l7jApADeDAEcMkE90JnUmmGl4USKQ0xhoW3JB5XY0YrxYWhLwMZZypUyjDGH35AbNwgUGiFBPpuGbHCpAOV1ZGXf2f/taftAv31DyeymN2d1IhAFAwTOmnzF/kKcdh3me7CYCOVNgycju84u8DeVlwfFq9/ZlTfldYrMUjOlrkjkD+rU+WzCROkcEchIDHR011syZW9JHD7y07N6JvhWMpz3pugaTkB6lWFVCKkhck0zzeMp2utq+uHrmfxOgoCO/Z8CXPlEQ1bdH8wgvhSIkEG0ICcQeExIFGdimjvKka7btJFZuaXOammIGKUCFQ53j9EN1dYKWqHf0t2w407W2tgs6h89ZnImjB55flh81tt9XirjjDuSl+oIPRQ0iWPgNZ5GqTqbBe3vSzEl5n5PhWKwocyR2HlqYN61qV18WjYjE8JLARZPQsUSim8foIRYTlGr02Ly7piASFRtKJ4VfieYhxdS2JcDVMN6xVOKZyrCGm8b108lrLRVzvptLH7IoEFLFANes6KnDi+uxfmvFnF17oALq5u1agu3/YfHkcSFzeSggV5eXRfIB7CHNcO5SUI+Ih5Ir7f4MAV9IqdFzdZgNpZw1Gcs1mNvgGbTbqQ9/cz7ZuuhgyYRQ49ljTyWHhr2DwpNHHFf+5gnWZ3Bharo+0TD5dNMw5vv9RlVpSRDHK4TlnoukhtYApuOHejSZQuo5g/A9BysdKRCyLl6062fN37OXMDlvUJtUrtmxo0avrW3wTrYs3jJ9RvRVChrmSmanPMpX2OXMsmDGh6AiEIwBAlvkOqIdBy+8JyAz8pz7QxiDth4KDy5uAlwzrWTnwC8Vc4KVAMZ3YUZ+IqoIjP3h5KFFX1ZMy3uW+7RhEDHgTi0zC9rS7uhPCDiNrGFyqBeERtKN/B0YlyFCkw0NJ5C0Ojv7zvT1a1WV1TuvZDdL4NTgB7CASYpsen6gqvG5jmTf5qHedADgkBl3D0nkSgNhZACDyi0FUKZRr3IdRjgN4WPPoFMIIegIK3mqd38fS80mcJKelM4szNyzZtQbkchGePuBRS8Eg9pHU8ojRQpSqs+ajAIwTjjUMQ/nvTNM0kicwYxZIYMh/891DYi+fvedB+c1xsm4lDU6ya+Axtz+RiAzEVYbajQOpq17F0R9QevNcEhfcU+xvyQQUalGJBSesqOkgPQ4YNyUZL9fSvUPDjoNAwN8/dwFjaczNkc3ptaMud1EIDtGcmXTcefO2cGSvKIFfp/2JIJxlq7xEl3nVPM4fDeIbPkD16/ptNc0bDu7qxbsu0R2JGywWMIjF2ft3tjfloAyQAGXiOn8hrqwbVvMXzaO+QeHXP6nF0wvX74Hf4NGG5GPjSlYoyM3P/0FbCT6zvM/yYoAAAAASUVORK5CYII=" role="presentation" style="pointer-events: none;" width="16" height="16">
          </div>
          <div class="js-toggle-track-x">
            <img src="data:image/png;base64,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" role="presentation" style="pointer-events: none;" width="16" height="16">
              </div>
              </div>
              <div class="js-toggle-thumb"></div>
                <input class="js-toggle-screenreader-only" type="checkbox" aria-label="Switch between Dark and Light mode">
                  </div>
                  </div>
  
  <style>

 

.js-toggle-wrapper {
    display: table;
    margin: 0 auto;
}

.js-toggle {
    touch-action: pan-x;
    display: inline-block;
    position: relative;
    cursor: pointer;
    background-color: transparent;
    border: 0;
    padding: 0;
    -webkit-touch-callout: none;
    user-select: none;
    -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
    -webkit-tap-highlight-color: transparent;
  }
  
  .js-toggle-screenreader-only {
    border: 0;
    clip: rect(0 0 0 0);
    height: 1px;
    margin: -1px;
    overflow: hidden;
    padding: 0;
    position: absolute;
    width: 1px;
  }
  
  .js-toggle-track {
    width: 50px;
    height: 24px;
    padding: 0;
    border-radius: 30px;
    background-color: hsl(222, 14%, 7%);
    transition: all 0.2s ease;
  }
  
  .js-toggle-track-check {
    position: absolute;
    width: 17px;
    height: 17px;
    left: 5px;
    top: 0px;
    bottom: 0px;
    margin-top: auto;
    margin-bottom: auto;
    line-height: 0;
    opacity: 0;
    transition: opacity 0.25s ease;
  }
  
  .js-toggle--checked .js-toggle-track-check {
    opacity: 1;
    transition: opacity 0.25s ease;
  }
  
  .js-toggle-track-x {
    position: absolute;
    width: 17px;
    height: 17px;
    right: 5px;
    top: 0px;
    bottom: 0px;
    margin-top: auto;
    margin-bottom: auto;
    line-height: 0;
    opacity: 1;
    transition: opacity 0.25s ease;
  }
  
  .js-toggle--checked .js-toggle-track-x {
    opacity: 0;
  }
  
  .js-toggle-thumb {
    position: absolute;
    top: 1px;
    left: 1px;
    width: 22px;
    height: 22px;
    border-radius: 50%;
    background-color: #fafafa;
    box-sizing: border-box;
    transition: all 0.5s cubic-bezier(0.23, 1, 0.32, 1) 0ms;
    transform: translateX(0);
  }
  
  .js-toggle--checked .js-toggle-thumb {
    transform: translateX(26px);
    border-color: #19ab27;
  }
  
  .js-toggle--focus .js-toggle-thumb {
    box-shadow: 0px 0px 2px 3px rgb(255, 167, 196);
  }
  
  .js-toggle:active .js-toggle-thumb {
    box-shadow: 0px 0px 5px 5px rgb(255, 167, 196);
  }
  
  body.dark-mode , 
  body.dark-mode main * {
    background-color: #000000;
    color: #000000;
    filter: invert(0.95);
}

  .dark-mode img {
  filter: invert(1) contrast(1.3) saturate(1.4);
}

  .dark-mode p {
  color: #000000;
}
  
  .dark-mode code {
    filter: invert(1) contrast(1.3) saturate(1.4);
    font-weight: bold;
    background: #353737;
    color: white;
}

  .dark-mode strong, b {
    filter: invert(1) contrast(1.3) saturate(1.4);
  }
  
  .dark-mode #social-share ul{
    margin: 0;
    filter: invert(1) contrast(1.3) saturate(1.4);
  }

  .dark-mode #header {
    filter: invert(1) contrast(1.3) saturate(1.4);
    position: fixed;
  }

  .dark-mode .js-toggle--focus .js-toggle-thumb {
    filter: invert(1) contrast(1.3) saturate(1.4);
  }

  .dark-mode .footer {
    filter: invert(1) contrast(1.3) saturate(1.4);
    position: fixed;
    background-color: white;
    font-weight: bold;
  }

  .dark-mode .nav-secondary {
    filter: invert(1) contrast(1.3) saturate(1.4);
}

  .dark-mode #reactions .reaction-items .reaction-item .reaction-item__button img, #reactions-promotion .reaction-items .reaction-item .reaction-item__button img {
    filter: invert(1) contrast(1.3) saturate(1.4);
}

</style>

<script>
    var body = document.body;
    var article = document.article;
	var switcher = document.getElementsByClassName('js-toggle')[0];

	
	switcher.addEventListener("click", function() {
        this.classList.toggle('js-toggle--checked');
        this.classList.add('js-toggle--focus');
		
		if (this.classList.contains('js-toggle--checked')) {
			body.classList.add('dark-mode');
			article.classList.add('post.dark-mode');
			
			localStorage.setItem('darkMode', 'true');
		} else {
			body.classList.remove('dark-mode');
			article.classList.remove('post.dark-mode');
			setTimeout(function() {
				localStorage.removeItem('darkMode');
			}, 100);
		}
	})

	
	if (localStorage.getItem('darkMode')) {
		
        switcher.classList.add('js-toggle--checked');
        body.classList.add('dark-mode');
        article.classList.add('post.dark-mode');
	}

</script>


      
        <script type="application/ld+json">
    {
        "@context" : "http://schema.org",
        "@type" : "BlogPosting",
        "mainEntityOfPage": {
             "@type": "WebPage",
             "@id": "https:\/\/algotech.netlify.com\/"
        },
        "articleSection" : "blog",
        "name" : "Introduction to Hierarchical Clustering",
        "headline" : "Introduction to Hierarchical Clustering",
        "description" : "Clustering merupakan salah satu metode Unsupervised Learning yang bertujuan untuk melakukan pengelompokan data berdasasrkan kemiripan\/jarak antar data. Clustering memiliki karakteristik dimana anggota dalam satu cluster memiliki kemiripan yang sama atau jarak yang sangat dekat, sementara anggota antar cluster memiliki kemiripan yang sangat berbeda atau jarak yang sangat jauh. Menurut (Tan et al., 2006) dalam bukunya yang berjudul Introduction to Data Mining, metode clustering dibagi menjadi dua jenis, yaitu Hierarchical Clustering dan Partitional Clustering1.",
        "inLanguage" : "en",
        "author" : "",
        "creator" : "",
        "publisher": "",
        "accountablePerson" : "",
        "copyrightHolder" : "",
        "copyrightYear" : "2021",
        "datePublished": "2021-02-22 00:00:00 \u002b0000 UTC",
        "dateModified" : "2021-02-22 00:00:00 \u002b0000 UTC",
        "url" : "https:\/\/algotech.netlify.com\/blog\/introduction-to-hierarchical-clustering\/",
        "wordCount" : "4777",
        "keywords" : [ "Machine Learning","Unsupervised Learning","Hierarchical Clustering","Blog" ]
    }
    </script>
        
            
                <title>Introduction to Hierarchical Clustering</title>
            
        

        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <meta name="generator" content="Hugo 0.87.0" />
        
  
    
  

  

  <link rel="apple-touch-icon-precomposed" href='https://algotech.netlify.com/favicon/apple-touch-icon-precomposed.png'>
  <link rel="icon" href='https://algotech.netlify.com/favicon/favicon.png'>
  
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content='/favicon/mstile.png'>
  <meta name="application-name" content="Algoritma Technical Blog">
  <meta name="msapplication-tooltip" content="To learn more about our approach to data science problems, feel free to hop over to our blog.">
  <meta name="msapplication-config" content='/favicon/ieconfig.xml'>



        
            <meta name="author" content="Inayatus S &amp; Nabiilah A F">
        
        
            
                <meta name="description" content="To learn more about our approach to data science problems, feel free to hop over to our blog.">
            
        

        <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Introduction to Hierarchical Clustering"/>
<meta name="twitter:description" content="Clustering merupakan salah satu metode Unsupervised Learning yang bertujuan untuk melakukan pengelompokan data berdasasrkan kemiripan/jarak antar data. Clustering memiliki karakteristik dimana anggota dalam satu cluster memiliki kemiripan yang sama atau jarak yang sangat dekat, sementara anggota antar cluster memiliki kemiripan yang sangat berbeda atau jarak yang sangat jauh. Menurut (Tan et al., 2006) dalam bukunya yang berjudul Introduction to Data Mining, metode clustering dibagi menjadi dua jenis, yaitu Hierarchical Clustering dan Partitional Clustering1."/>
<meta name="twitter:site" content="@teamalgoritma"/>

        <meta property="og:title" content="Introduction to Hierarchical Clustering" />
<meta property="og:description" content="Clustering merupakan salah satu metode Unsupervised Learning yang bertujuan untuk melakukan pengelompokan data berdasasrkan kemiripan/jarak antar data. Clustering memiliki karakteristik dimana anggota dalam satu cluster memiliki kemiripan yang sama atau jarak yang sangat dekat, sementara anggota antar cluster memiliki kemiripan yang sangat berbeda atau jarak yang sangat jauh. Menurut (Tan et al., 2006) dalam bukunya yang berjudul Introduction to Data Mining, metode clustering dibagi menjadi dua jenis, yaitu Hierarchical Clustering dan Partitional Clustering1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://algotech.netlify.com/blog/introduction-to-hierarchical-clustering/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2021-02-22T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-02-22T00:00:00+00:00" />


        <meta property="og:image" content="https://algotech.netlify.com//images/logo.png">
        <meta property="og:image:type" content="image/png">
        <meta property="og:image:width" content="512">
        <meta property="og:image:height" content="512">
        <meta itemprop="name" content="Introduction to Hierarchical Clustering">
<meta itemprop="description" content="Clustering merupakan salah satu metode Unsupervised Learning yang bertujuan untuk melakukan pengelompokan data berdasasrkan kemiripan/jarak antar data. Clustering memiliki karakteristik dimana anggota dalam satu cluster memiliki kemiripan yang sama atau jarak yang sangat dekat, sementara anggota antar cluster memiliki kemiripan yang sangat berbeda atau jarak yang sangat jauh. Menurut (Tan et al., 2006) dalam bukunya yang berjudul Introduction to Data Mining, metode clustering dibagi menjadi dua jenis, yaitu Hierarchical Clustering dan Partitional Clustering1."><meta itemprop="datePublished" content="2021-02-22T00:00:00+00:00" />
<meta itemprop="dateModified" content="2021-02-22T00:00:00+00:00" />
<meta itemprop="wordCount" content="4777">
<meta itemprop="keywords" content="Machine Learning,Unsupervised Learning,Hierarchical Clustering," />
        

        
            
        

        
        
            <link disabled id="dark-mode-theme" rel="stylesheet" href="https://algotech.netlify.com/css/dark.css">
            <link rel="stylesheet" href="https://algotech.netlify.com/css/monokai-sublime.css">
            <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway:400,800,900|Source+Sans+Pro:400,700">
            <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.1.25/jquery.fancybox.min.css">
            <link rel="stylesheet" href="https://algotech.netlify.com/css/main.css">
            <link rel="stylesheet" href="https://algotech.netlify.com/css/add-on.css">
            <link rel="stylesheet" href="https://algotech.netlify.com/css/academicons.min.css">
            <link href="https://algotech.netlify.com/lib/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet">
            <link href="https://algotech.netlify.com/lib/dt-core-1.10.16/css/jquery.dataTables.min.css" rel="stylesheet">
            <link href="https://algotech.netlify.com/lib/dt-core-1.10.16/css/jquery.dataTables.extra.css" rel="stylesheet">
            <link href="https://algotech.netlify.com/lib/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet">
        
            <link rel="stylesheet" href=
            "https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
                <link rel="stylesheet" href=
            "https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
                      integrity=
            "sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" 
                      crossorigin="anonymous">
                <script src=
            "https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js">
                </script>
                <script src=
            "https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js">
                </script>
                <script src="https://cdnjs.cloudflare.com/ajax/libs/js-cookie/2.1.0/js.cookie.js">
                </script>
                
                <script src="//cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.0/jquery.cookie.min.js">
                </script>
              
                <style>
                    
                     


                    .modal:before {
                        content: '';
                        display: inline-block;
                        height: 20%;
                        vertical-align: middle;
                    }
                      
                    .modal-dialog {
                        
                    display: -webkit-box;
                    display: -webkit-flex;
                    display: -ms-flexbox;
                    display: flex;
                    -webkit-box-orient: vertical;
                    -webkit-box-direction: normal;
                    -webkit-flex-direction: column;
                        -ms-flex-direction: column;
                            flex-direction: column;
                    -webkit-box-pack: center;
                    -webkit-justify-content: center;
                        -ms-flex-pack: center;
                            justify-content: center;
                    }

                      
                    .modal .modal-content {
                        padding: 20px 20px 20px 20px;
                        -webkit-animation-name: modal-animation;
                        -webkit-animation-duration: 0.5s;
                        animation-name: modal-animation;
                        animation-duration: 1.5s;
                    }

                

                    .modal .leadin-button {
                        -webkit-border-radius: 0.25em;
                        -moz-border-radius: 0.25em;
                        -ms-border-radius: 0.25em;
                        -o-border-radius: 0.25em;
                        border-radius: 0.25em;
                        -webkit-appearance: none;
                        cursor: pointer;
                        font-size: 1em;
                        font-weight: bold;
                        line-height: 1;
                        padding: 1em 1.5em;
                        width: 100%;
                        text-decoration: none;
                    }
                    .modal .leadin-button-primary {
                        background: #a90606 !important;
                        color: #FFFFFF !important;
                    }
                      
                    @-webkit-keyframes modal-animation {
                        from {
                            top: -100px;
                            opacity: 0;
                        }
                        to {
                            top: 0px;
                            opacity: 1;
                        }
                    }
                      
                    @keyframes modal-animation {
                        from {
                            top: -100px;
                            opacity: 0;
                        }
                        to {
                            top: 0px;
                            opacity: 1;
                        }
                    }
                </style>

            <script src="content/bootstrapJS/jquery-2.1.1.min.js" type="text/javascript"></script>

            <script type="text/javascript">
                
                
                $( document ).ready(function() {
  if (document.cookie.indexOf('visited=true') == -1){
    
                      $('#signupModal').modal('show');
    
                  var year = 1000*60*60*24*365;
    var expires = new Date((new Date()).valueOf() + year);
    document.cookie = "visited=true;expires=" + expires.toUTCString();

  }
}); 

            </script>
        
            

        
            
                
            
                
                    <link rel="stylesheet" href="https://algotech.netlify.com/css/main.css">
                
            
        


  
    
      <link rel="stylesheet" href="https://algotech.netlify.com/css/monokai-sublime.css" rel="stylesheet" id="theme-stylesheet">
      <script src="https://algotech.netlify.com/js/highlight.pack.js"></script>
      <script>hljs.initHighlightingOnLoad();</script>
  


      
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-164959107-2', 'auto');
	
	ga('send', 'pageview');
}
</script>



      
    </head>
    <body>

        <div class="modal" id="signupModal"
        role="dialog" aria-labelledby="myModalLabel"
        aria-hidden="true">
 
       <div class="modal-dialog">
           <div class="modal-content">
 
               
               <div class="m-header">
                   <button class="close" data-dismiss="modal">
                       Ã—
                   </button>
                   <h2 class="myModalLabel"> Upcoming Workshop! </h2>
               </div>
 
               
               <div class="inputs">
 
                <a href="https://algorit.ma/ds-course/dss-trading/?utm_source=algotech"><img src="https://algotech.netlify.com/img/2021/ads/dss_okt.png",
                     alt="Building Algorithmic Trading Dashboard", style="float: left; padding-right: 20px;" padding: 1px  width="30%" height="30%"></a>
                <p>This 3-day online workshop is a beginner-friendly introduction to algorithmic trading using the Python programming language. Throughout the online course, we will provide you with hands-on examples and a rich interactive experience. One Instructor and two Teaching Assistants will help you to troubleshoot or help with any difficulties encountered.</p>
                <div class="advance-wrapper callout-special-font">
                    <a href="https://algorit.ma/ds-course/dss-trading/?utm_source=algotech" target="_blank" 
                    class="leadin-button leadin-advance-button leadin-button-primary">LEARN MORE</a>
                </div>
               </div>
 
 
           </div>
       </div>
   </div>

    </body>

      
      <div id="wrapper">

    
    
<header id="header">
    
      <h1><a href="https://algotech.netlify.com/">blog</a></h1>
    

    <nav class="links">
        <ul>
            
                <li>
                    <a href="https://algotech.netlify.com/">
                            <i class="fa fa-home">&nbsp;</i>Home
                    </a>
                </li>
            
                <li>
                    <a href="https://algotech.netlify.com/tags/machine-learning/">
                            <i class="fa fa-cog">&nbsp;</i>Machine Learning
                    </a>
                </li>
            
                <li>
                    <a href="https://algotech.netlify.com/tags/data-visualization/">
                            <i class="fa fa-area-chart">&nbsp;</i>Data Visualization
                    </a>
                </li>
            
                <li>
                    <a href="https://algotech.netlify.com/tags/">
                            <i class="fa fa-list">&nbsp;</i>Article List
                    </a>
                </li>
            
        </ul>
    </nav>
    <nav class="main">
        <ul>
            
            <li id="share-nav" class="share-menu" style="display:none;">
                <a class="fa-share-alt" href="#share-menu">Share</a>
            </li>
            
            <li class="search">
                <a class="fa-search" href="#search">Search</a>
                <form id="search" method="get" action="//google.com/search">
                    <input type="text" name="q" placeholder="Search" />
                    <input type="hidden" name="as_sitesearch" value="https://algotech.netlify.com/">
                </form>
            </li>
            <li class="menu">
                <a class="fa-bars" href="#menu">Menu</a>
            </li>
        </ul>
    </nav>
</header>


<section id="menu">

    
        <section>
            <form class="search" method="get" action="//google.com/search">
                <input type="text" name="q" placeholder="Search" />
                <input type="hidden" name="as_sitesearch" value="https://algotech.netlify.com/">
            </form>
        </section>

    
        <section>
            <ul class="links">
                
                    <li>
                        <a href="https://algotech.netlify.com/">
                            <h3>
                                <i class="fa fa-home">&nbsp;</i>Home
                            </h3>
                        </a>
                    </li>
                
                    <li>
                        <a href="https://algotech.netlify.com/tags/machine-learning/">
                            <h3>
                                <i class="fa fa-cog">&nbsp;</i>Machine Learning
                            </h3>
                        </a>
                    </li>
                
                    <li>
                        <a href="https://algotech.netlify.com/tags/data-visualization/">
                            <h3>
                                <i class="fa fa-area-chart">&nbsp;</i>Data Visualization
                            </h3>
                        </a>
                    </li>
                
                    <li>
                        <a href="https://algotech.netlify.com/tags/">
                            <h3>
                                <i class="fa fa-list">&nbsp;</i>Article List
                            </h3>
                        </a>
                    </li>
                
            </ul>
        </section>

    
        <section class="recent-posts">
            <div class="mini-posts">
                <header>
                    <h3>Recent Posts</h3>
                </header>
                

                
                    
                

                
                        <article class="mini-post">
                            <header>
                                <h3><a href="https://algotech.netlify.com/blog/regression-arima-arimax/">Regression ARIMA (ARIMAX)</a></h3>
                                
                                <time class="published" datetime=
                                    '2021-09-09'>
                                    September 9, 2021</time>
                            </header>
                            

                        </article>
                
                        <article class="mini-post">
                            <header>
                                <h3><a href="https://algotech.netlify.com/blog/hotel-forecast/">Multiple Hotel Segments Demand Forecasting</a></h3>
                                
                                <time class="published" datetime=
                                    '2021-07-30'>
                                    July 30, 2021</time>
                            </header>
                            

                        </article>
                
                        <article class="mini-post">
                            <header>
                                <h3><a href="https://algotech.netlify.com/blog/advancing-your-shinyapp-ii/">Advancing Your Shiny Application II</a></h3>
                                
                                <time class="published" datetime=
                                    '2021-06-03'>
                                    June 3, 2021</time>
                            </header>
                            

                        </article>
                
                        <article class="mini-post">
                            <header>
                                <h3><a href="https://algotech.netlify.com/blog/gridsearchcv/">GridSearchCV</a></h3>
                                
                                <time class="published" datetime=
                                    '2021-05-22'>
                                    May 22, 2021</time>
                            </header>
                            

                        </article>
                
                        <article class="mini-post">
                            <header>
                                <h3><a href="https://algotech.netlify.com/blog/kemiripan-teks/">Pengenalan Kemiripan Teks (Text Similarity) di Python</a></h3>
                                
                                <time class="published" datetime=
                                    '2021-05-20'>
                                    May 20, 2021</time>
                            </header>
                            

                        </article>
                

                
                    <a href=
                        
                            /blog/
                        
                        class="button">View more posts</a>
                
            </div>
        </section>

    
        
</section>

    <section id="share-menu">
    <section id="social-share-nav">
        <ul class="links">
            <header>
                <h3>Share this post <i class="fa fa-smile-o"></i></h3>
            </header>
            



<li>
  <a href="https://twitter.com/intent/tweet?text=Introduction%20to%20Hierarchical%20Clustering by Inayatus%20S%20%26%20Nabiilah%20A%20F&amp;url=https%3a%2f%2falgotech.netlify.com%2fblog%2fintroduction-to-hierarchical-clustering%2f" target="_blank" class="share-btn twitter">
    <i class="fa fa-twitter"></i>
    <p>Twitter</p>
    </a>
</li>








<li>
  <a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2falgotech.netlify.com%2fblog%2fintroduction-to-hierarchical-clustering%2f" target="_blank" class="share-btn facebook">
    <i class="fa fa-facebook"></i>
    <p>Facebook</p>
    </a>
</li>







<li>
  <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2falgotech.netlify.com%2fblog%2fintroduction-to-hierarchical-clustering%2f&amp;title=Introduction%20to%20Hierarchical%20Clustering" target="_blank" class="share-btn linkedin">
      <i class="fa fa-linkedin"></i>
      <p>LinkedIn</p>
    </a>
</li>











        </ul>
    </section>
</section>

    
    <div id="main">
        
        
        <article class="post">
  <header>
    <div class="title">
        
            <h2><a href="https://algotech.netlify.com/blog/introduction-to-hierarchical-clustering/">Introduction to Hierarchical Clustering</a></h2>
        
        
    </div>
    <div class="meta">
        

        <time class="published"
            datetime='2021-02-22'>
            February 22, 2021</time>
        <span class="author"><a href="https://github.com/inytss/hierarchical-clustering">Inayatus S &amp; Nabiilah A F</a></span>
        
            <p>23 minute read</p>
        
        
    </div>
</header>


  
    <section id="social-share">
      <ul class="icons">
        



<li>
  <a href="https://twitter.com/intent/tweet?text=Introduction%20to%20Hierarchical%20Clustering by Inayatus%20S%20%26%20Nabiilah%20A%20F&amp;url=https%3a%2f%2falgotech.netlify.com%2fblog%2fintroduction-to-hierarchical-clustering%2f" target="_blank" class="share-btn twitter">
    <i class="fa fa-twitter"></i>
    <p>Twitter</p>
    </a>
</li>








<li>
  <a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2falgotech.netlify.com%2fblog%2fintroduction-to-hierarchical-clustering%2f" target="_blank" class="share-btn facebook">
    <i class="fa fa-facebook"></i>
    <p>Facebook</p>
    </a>
</li>







<li>
  <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2falgotech.netlify.com%2fblog%2fintroduction-to-hierarchical-clustering%2f&amp;title=Introduction%20to%20Hierarchical%20Clustering" target="_blank" class="share-btn linkedin">
      <i class="fa fa-linkedin"></i>
      <p>LinkedIn</p>
    </a>
</li>











      </ul>
    </section>
  

  
    

    
        
        







  
  
    
  


        
        
        

        <a href="https://algotech.netlify.com/blog/introduction-to-hierarchical-clustering/" class="image featured">
            <img src="https://algotech.netlify.com/img/2021/02/hc.png" alt="">
        </a>
    


  <div id="content">
    <p>Clustering merupakan salah satu metode Unsupervised Learning yang bertujuan untuk melakukan pengelompokan data berdasasrkan kemiripan/jarak antar data. Clustering memiliki karakteristik dimana anggota dalam satu cluster memiliki kemiripan yang sama atau jarak yang sangat dekat, sementara anggota antar cluster memiliki kemiripan yang sangat berbeda atau jarak yang sangat jauh. Menurut (Tan et al., 2006) dalam bukunya yang berjudul <em>Introduction to Data Mining</em>, metode clustering dibagi menjadi dua jenis, yaitu <strong>Hierarchical Clustering</strong> dan <strong>Partitional Clustering</strong><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p><strong>Partitional Clustering</strong> umumnya bertujuan untuk mengelompokkan data menjadi beberapa cluster yang lebih kecil<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Pada prosesnya, setiap cluster akan memiliki titik pusat cluster (<em>centroid</em>) dan mencoba menghitung setiap data yang paling dekat dengan centroid tersebut. Metode dalam partitional clustering diantaranya <em>k-means</em>, <em>fuzzy k-means</em>, dan <em>mixture modeling</em>.</p>
<!-- raw HTML omitted -->
<p>Sedangkan dalam <strong>Hierarchical Clustering</strong>, pengelompokan data dilakukan dengan membuat suatu bagan hirarki (<strong>dendrogram</strong>) dengan tujuan menunjukkan kemiripan antar data. Setiap data yang mirip akan memiliki hubungan hirarki yang dekat dan menbentuk cluster data. Bagan hirarki akan terus terbentuk hingga seluruh data terhubung dalam bagan hirarki tersebut. Cluster dapat dihasilkan dengan memotong bagan hirarki pada level tertentu. Beberapa metode dalam hierarchical clustering yaitu <em>single linkage</em>, <em>complete linkage</em>, <em>average linkage</em>, dan <em>ward&rsquo;s minimum variance</em>.</p>
<!-- raw HTML omitted -->
<p>Pada kesempatan kali ini kita akan mendalami terkait <strong>Hierarchical Clustering</strong> serta aplikasinya untuk pengolahan data.</p>
<h1 id="hierarchical-clustering">Hierarchical Clustering</h1>
<h2 id="approach">Approach</h2>
<p>Secara umum, <em>hierarchical clustering</em> dibagi menjadi dua jenis yaitu <strong>agglomerative</strong> dan <strong>divisive</strong><sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. Kedua metode ini dibedakan berdasarkan pendekatan dalam melakukan pengelompokkan data hingga membentuk dendrogram, menggunakan <em>bottom-up</em> atau <em>top-down manner</em>.</p>
<ol>
<li><strong>Agglomerative Clustering</strong></li>
</ol>
<p><em>Agglomerative clustering</em> biasa disebut juga sebagai agglomerative nesting (<strong>AGNES</strong>) dimana cara kerja dalam melakukan pengelompokan data menggunakan <strong>bottom-up manner</strong>. Prosesnya dimulai dengan menganggap setiap data sebagai satu cluster kecil (<em>leaf</em>) yang hanya memiliki satu anggota saja, lalu pada tahap selanjutnya dua cluster yang memiliki kemiripan akan dikelompokkan menjadi satu cluster yang lebih besar (<em>nodes</em>). Proses ini akan dilakukan terus menerus hingga semua data menjadi satu cluster besar (<em>root</em>).</p>
<ol start="2">
<li><strong>Divisive hierarchical clustering</strong></li>
</ol>
<p><em>Divisive hierarchical clustering</em> biasa disebut juga sebagai divisive analysis (<strong>DIANA</strong>) di mana cara kerja dalam melakukan pengelompokan data menggunakan <strong>top-down manner</strong>. Prosesnya dimulai dengan menganggap satu set data sebagai satu cluster besar (<em>root</em>), lalu dalam setiap iterasinya setiap data yang memiliki karakteristik yang berbeda akan dipecah menjadi dua cluster yang lebih kecil (<em>nodes</em>) dan proses akan terus berjalan hingga setiap data menjadi satu cluster kecil (<em>leaf</em>) yang hanya memiliki satu anggota saja.</p>
<p>Berikut adalah ilustrasi mengenai bagaimana agglomerative dan divisive clustering bekerja.</p>
<!-- raw HTML omitted -->
<p>Selain memahami pendekatan dalam pembuatan dendrogram, mari memahami bagaimana setiap cluster dapat dibuat dan digabungkan.</p>
<h2 id="dissimilarity-measure">(Dis)similarity Measure</h2>
<p>Tujuan dari clustering secara umum, baik hierarchical maupun partitional clustering adalah untuk membuat cluster yang memiliki karakteristik yang sama dalam satu anggota cluster dan memiliki karakteristik yang berbeda antar clusternya. Konsep inilah yang mengharuskan proses pembuatan cluster memperhatikan jarak / <strong>(dis)similarity</strong> / ukuran ketidakmiripan antar data.</p>
<p>Terdapat beragam metode penghitungan (dis)similarity. Pemilihan metode (dis)similarity akan menentukan bagaimana kemiripan antar data dihitung. Itulah mengapa pemilihan metode (dis)similarity menjadi salah satu hal penting dalam pembuatan hierarchical clustering.</p>
<p>Metode penghitungan (dis)similarity yang sering digunakan adalah <em>euclidean distance</em> dan <em>manhattan distance</em>, namun bisa saja menggunakan pengukuran jarak yang lain, bergantung pada data yang sedang kita analisis. Berikut ini formula dalam perhitungan (dis)similarity dari kedua metode tersebut:</p>
<ol>
<li><strong>Euclidean distance</strong></li>
</ol>
<p><code>$$d_{xy} = \sqrt {\sum_{i=1}^{n}(x_i - y_i)^2}$$</code></p>
<ol start="2">
<li><strong>Manhattan distance</strong></li>
</ol>
<p><code>$$d_{xy} = \sum_{i=1}^{n} |{(x_i - y_i)}|$$</code></p>
<p>Ada beberapa pengukuran (dis)similarity yang lain yang bisa digunakan yaitu menggunakan <strong>correlation-based distance</strong>. Correlation-based distance biasa digunakan ketika kita ingin mengetahui bentuk (dis)similarity pada suatu data yang bergerak &ldquo;naik&rdquo; atau &ldquo;turun&rdquo; secara bersamaan. Pengukuran (dis)similarity ini sering digunakan untuk melakukan analisis ekspresi gen atau dalam dunia marketing, ketika kita ingin melakukan <em>customer segmentation</em> berdasarkan kesamaan barang yang dibeli oleh pelanggan tanpa memperhatikan banyak barang yang mereka beli.</p>
<p><em>Euclidean distance</em> dan <em>manhattan distance</em> cenderung memiliki konsep yang berkebalikan dengan correlation-based distance, data yang akan dikelompokkan bersama merupakan data yang memiliki karakteristik nilai yang sama, entah sama besarnya atau sama kecilnya. Pengukuran ini biasa digunakan pada kasus <em>customer segmentation</em> yang memperhatikan banyaknya pembelian dari pelanggan, segmentasi daerah yang memiliki kasus COVID tinggi/rendah, dan lain sebagainya<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p>Pada R, kita dapat menggunakan fungsi <code>dist()</code> untuk menghitung (dis)similarity antar data. Secara default, fungsi <code>dist()</code> akan menghitung <em>euclidean distance</em> antar data.</p>
<p>Selanjutnya, nilai (dis)similarity antar data ini akan dibentuk menjadi <strong>distance matrix</strong>. Kemudian, <em>distance matrix</em> tersebut akan diolah untuk penyusunan dendrogram.</p>
<h2 id="linkage-method">Linkage Method</h2>
<p>Dalam hierarchical clustering, selain menghitung (dis)similarity antar data, diperlukan juga cara untuk menghitung (dis)similarity antar cluster sehingga dapat terbentuk dendrogram dari cluster-cluster yang dekat. Proses penggabungan cluster-cluster kecil menjadi satu dendrogram utuh dilakukan melalui beberapa pendekatan <strong>Linkage Method</strong>. Berikut ini linkage method yang sering digunakan pada agglomerative approach:</p>
<ol>
<li><strong>Complete Linkage</strong> / <strong>Maximum Linkage</strong></li>
<li><strong>Single Linkage</strong> / <strong>Minimum Linkage</strong></li>
<li><strong>Average Linkage</strong></li>
<li><strong>Centroid Linkage</strong></li>
<li><strong>Ward&rsquo;s minimum Variance</strong></li>
</ol>
<h3 id="completemaximum-linkage">Complete/Maximum Linkage</h3>
<p>Pengukuran (dis)similarity atau jarak antar cluster dilakukan dengan mengukur terlebih dahulu jarak antar tiap observasi dari cluster yang berbeda (<strong>pairwise distances</strong>). Jarak paling tinggi (<em>maximum distance</em>) akan menjadi ukuran (dis)similarity antar cluster. Kemudian, dendrogram akan terbentuk dari cluster-cluster yang memiliki (dis)similarity paling kecil. Hal ini membuat dendrogram yang terbentuk menjadi lebih terpisah antar clusternya (terbentuk cluster yang &ldquo;compact&rdquo;).</p>
<p>Berikut formula jarak antar cluster menggunakan complete linkage:</p>
<p><code>$$d_{12} = \max_{ij} d(X_i, Y_j)$$</code></p>
<p>di mana:</p>
<ul>
<li><code>\(X_1, X_2, ..., X_k\)</code> : observasi pada cluster 1</li>
<li><code>\(Y_1, Y_2, ..., Y_k\)</code> : observasi pada cluster 2</li>
<li><code>\(d(X, Y)\)</code> : jarak antara data pada cluster 1 dengan data pada cluster 2</li>
</ul>
<h3 id="singleminimum-linkage">Single/Minimum Linkage</h3>
<p>Pengukuran (dis)similarity atau jarak antar cluster dilakukan dengan mengukur terlebih dahulu jarak antar tiap observasi dari cluster yang berbeda pairwise distances. Jarak paling kecil (<em>minimum distance</em>) akan menjadi ukuran (dis)similarity antar cluster. Dendrogram akan terbentuk dari cluster-cluster yang memiliki (dis)similarity paling kecil. Hal ini membuat dendrogram yang terbentuk menjadi lebih &ldquo;loose&rdquo; atau berdekatan antar clusternya.</p>
<p>Berikut formula jarak antar cluster menggunakan single linkage:</p>
<p><code>$$d_{12} = \min_{ij} d(X_i, Y_j)$$</code></p>
<h3 id="average-linkage">Average Linkage</h3>
<p>Pengukuran (dis)similarity atau jarak antar cluster dilakukan dengan mengukur terlebih dahulu jarak antar tiap observasi dari cluster yang berbeda <em>pairwise distances</em>. Kemudian, dihitung rata-rata jarak dari <em>pairwise distance</em> tersebut dan nilai tersebut akan menjadi ukuran (dis)similarity antar cluster. Dendrogram akan terbentuk dari cluster-cluster yang memiliki (dis)similarity paling kecil. Umumnya metode ini akan menghasilkan cluster yang tidak terlalu &ldquo;loose&rdquo; maupun &ldquo;compact&rdquo;.</p>
<p>Berikut formula jarak antar cluster menggunakan average linkage:</p>
<p><code>$$d_{12} = \frac{1}{kl} \sum_{i=1}^{k}\sum_{j=1}^{l} d(X_i, Y_j)$$</code></p>
<h3 id="centroid-linkage">Centroid Linkage</h3>
<p>Perhitungan (dis)similarity atau jarak antar cluster dilakukan dengan mengukur jarak antar centroid pada dua cluster. Perhitungan centroid disini menggunakan rata-rata pada suatu variabel x. Dendrogram yang terbentuk akan berdasarkan cluster dengan jarak antar centroid paling kecil.</p>
<p>Berikut formula jarak antar cluster menggunakan centroid linkage:</p>
<p><code>$$d_{12} = d(\bar X, \bar Y)$$</code></p>
<h3 id="wards-minimum-variance">Ward&rsquo;s Minimum Variance</h3>
<p>Pada metode ini, di tiap iterasinya akan dibentuk cluster-cluster yang kemudian dihitung nilai <em>within sum of square</em> tiap cluster (WSS). WSS dapat diartikan sebagai jumlah dari jarak tiap observasi ke nilai tengah cluster. Cluster-cluster yang menghasilkan within sum of square terkecil akan diambil kemudian digabungkan hingga membentuk satu dendrogram utuh.</p>
<p>Berikut adalah ilustrasi untuk kelima jenis linkage di atas<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>:</p>
<!-- raw HTML omitted -->
<p>Linkage method akan menentukan rupa dari dendrogram yang terbentuk. Telah dilakukan diskusi terkait beragam linkage method dan pemilihannya <a href="https://stats.stackexchange.com/questions/195446/choosing-the-right-linkage-method-for-hierarchical-clustering">disini</a></p>
<h2 id="interpretasi-dendrogram">Interpretasi Dendrogram</h2>
<p>Setelah menghasilkan dendrogram dari hierarchical clustering, sudah sepatutnya kita perlu mampu membaca dendrogram tersebut. Sebagai contoh, berikut adalah gambar kedekatan beberapa instrumen musik yang digambarkan dengan dendrogram:</p>
<!-- raw HTML omitted -->
<p>Dendrogram sejatinya merupakan struktur yang menggambarkan kemiripan antar data. Tiap data pada awalnya diletakan di suatu level dasar dimana tiap data berdiri sendiri sebagai satu cluster tunggal. Pada contoh di atas, tiap instrumen musik merupakan satu cluster tunggal di bagian dasar dendrogram.</p>
<p>Kemudian, data-data terdekat akan digabungkan menjadi satu cluster dan dihubungkan dengan suatu garis. Contohnya cluster instrumen &ldquo;piccolo&rdquo; &amp; &ldquo;flute&rdquo; dan cluster instrumen &ldquo;basoon&rdquo; &amp; &ldquo;clarinet&rdquo;. Garis tersebut dimulai dari titik data dan bersatu di titik tertentu.</p>
<p><strong>Panjang garis antar data (dihitung tegak lurus dari titik data hingga titik kedua garis bersatu) mewakilkan nilai (dis)similarity antar data</strong>. Disini kita bisa menyimpulkan bahwa &ldquo;piccolo&rdquo; dengan &ldquo;flute&rdquo; memiliki kedekatan yang lebih tinggi dibandingkan &ldquo;basoon&rdquo; dengan &ldquo;clarinet&rdquo;. Kita juga bisa mengetahui bahwa &ldquo;flute&rdquo; memiliki kedekatan yang lebih tinggi dengan &ldquo;clarinet&rdquo; dibandingkan dengan &ldquo;trumpet&rdquo;. Hal ini karena &ldquo;flute&rdquo; dan &ldquo;clarinet&rdquo; memiliki panjang garis antar data yang lebih pendek dibandingkan antara &ldquo;flute&rdquo; dan &ldquo;trumpet&rdquo;.</p>
<p>Pada partitional clustering yang mungkin lebih umum kita kenal (contohnya k-means), pengguna perlu menentukan jumlah <code>k</code> cluster yang ingin dibentuk. Namun, pada hierarchical clustering hal tersebut tidak diwajibkan. Hal ini berangkat dari tujuan awal hierarchical clustering yang fokus pada mengetahui kedekatan antar data (bukan mengetahui partisi antar data).</p>
<p>Meskipun begitu, <strong>kita tetap dapat membuat cluster-cluster data dengan memotong dendrogram pada nilai (dis)similarity tertentu</strong>. Pada contoh dendrogram di atas, kita membuat partisi berupa cluster &ldquo;strings&rdquo;, &ldquo;woodwind&rdquo;, &ldquo;brass&rdquo;, dan &ldquo;percussion&rdquo;.</p>
<p>Layaknya menentukan kedekatan antar data, kita juga dapat menetukan kedekatan antar cluster dengan memperhitungkan panjang garis antar data. Dari dendrogram di atas, kita dapat menarik insight bahwa cluster &ldquo;woodwind&rdquo; lebih dekat dengan cluster &ldquo;strings&rdquo; dibandingkan dengan cluster &ldquo;brass&rdquo; dan &ldquo;percussion&rdquo;.</p>
<p>Bila disimpulkan, <strong>semakin panjang garis antar data maka semakin berbeda antar data/cluster tersebut, dan semakin pendek garis antar data maka semakin mirip antar data/cluster tersebut</strong>. Dari pemahaman ini, kita juga bisa menentukan manakah data yang berdekatan/berjauhan dengan data yang sedang kita analisis.</p>
<p>Hal lain yang dapat diperhatikan dalam analisis cluster adalah adanya kemungkinan untuk memperoleh cluster yang hanya terdiri dari satu atau sedikit sekali anggota. Ketika hal tersebut terjadi, kita perlu melakukan pengecekan kembali pada data yang kita miliki. Hal ini bisa disebabkan karena adanya data yang cukup berbeda dengan yang lainnya atau biasa disebut <strong>outlier / anomali</strong>.</p>
<h2 id="alur-pengerjaan">Alur Pengerjaan</h2>
<p>Secara umum, berikut adalah langkah-langkah yang dilakukan untuk melakukan hierarchical cluster analysis, menggabungkan konsep-konsep yang sudah kita pahami sebelumnya:</p>
<ol>
<li>Menyiapkan data dimana data yang digunakan adalah data bertipe numerik agar dapat digunakan untuk penghitungan jarak.</li>
<li>Menghitung (dis)similarity atau jarak antar data yang berpasangan pada dataset. Metode penghitungan (dis)similarity dapat dipilih berdasarkan data. Nilai (dis)similarity tersebut kemudian akan disusun menjadi distance matrix.</li>
<li>Membuat dendrogram dari distance matrix menggunakan linkage method tertentu. Kita juga dapat mencoba beberapa linkage method kemudian memilih dedrogram paling baik.</li>
<li>Menentukan dimana akan melakukan pemotongan tree (dengan nilai (dis)similarity tertentu). Disinilah tahap dimana cluster akan terbentuk.</li>
<li>Melakukan interpretasi dari dendrogram yang telah didapat.</li>
</ol>
<h2 id="pros--cons">Pros &amp; Cons</h2>
<p>Berikut adalah beberapa kelebihan dan kekurangan hierarchical clustering yang juga dapat dipertimbangkan sebelum dipakai untuk analisis data.</p>
<p><strong>Kelebihan:</strong></p>
<ul>
<li>Mampu menggambarkan kedekatan antar data dengan dendrogram.</li>
<li>Cukup mudah untuk pembuatannya.</li>
<li>Dapat menentukan banyak cluster yang terbentuk setelah dendrogram terbentuk.</li>
</ul>
<p><strong>Kekurangan:</strong></p>
<ul>
<li>Tidak dapat menganalisis data kategorik secara langsung (terhambat pada penghitungan jarak yang hanya bisa dilakukan untuk data numerik, sehingga data kategorik perlu dipre-process terlebih dahulu).</li>
<li>Tidak diperuntukkan untuk menghasilkan jumlah cluster optimal yang mutlak (jumlah cluster dapat berubah-ubah tergantung level pemotongan dendrogram).</li>
<li>Sensitif terhadap data yang memiliki skala berbeda (sehingga data perlu dinormalisasi/standarisasi terlebih dahulu).</li>
<li>Sensitif terhadap outlier.</li>
<li>Cukup berat komputasinya untuk data berukuran besar.</li>
</ul>
<h1 id="study-case-analisis-data-usarrests">Study Case: Analisis Data <code>USArrests</code></h1>
<p>Pada artikel ini, kita akan coba mengaplikasikan cara kerja hierarchical clustering AGNES untuk menganalisis kedekatan Negara Bagian AS berdasarkan tingkat kriminalitasnya. Kita akan menggunakan dataset <code>USArrest</code> yang menyimpan informasi statistik terkait jumlah kriminalitas <code>assault</code>, <code>murder</code> dan <code>rape</code> per 100,000 penduduk pada 50 negara bagian Amerika Serikat pada tahun 1973, berikut data populasi penduduk tiap negara bagian tersebut.</p>
<h2 id="dataset">Dataset</h2>
<p>Pertama, kita load terlebih dahulu data yang akan digunakan.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">head</span>(USArrests)
</code></pre></div><pre><code>#&gt;            Murder Assault UrbanPop Rape
#&gt; Alabama      13.2     236       58 21.2
#&gt; Alaska       10.0     263       48 44.5
#&gt; Arizona       8.1     294       80 31.0
#&gt; Arkansas      8.8     190       50 19.5
#&gt; California    9.0     276       91 40.6
#&gt; Colorado      7.9     204       78 38.7
</code></pre><h2 id="preparasi-data">Preparasi Data</h2>
<p>Sebelum membuat clustering, kita perlu menyiapkan dataset yang digunakan. Dari tampilan data sebelumnya, kita telah mengetahui bahwa tipe data pada tiap kolom sudah tepat/sesuai konteks kolomnya. Perlu diperhatikan bahwa, analisis clustering didasari oleh analisis jarak, sehingga dibutuhkan variable dengan tipe <strong>numerik</strong>. Variabel dengan tipe kategorik tidak terlalu bagus untuk dimasukkan pada analisis clustering. Pada dateset kita, semua variable bertipe numerik sehingga cocok untuk analisis clustering.</p>
<p>Selanjutnya kita dapat cek apakah terdapat <strong>missing value</strong> pada data. Dari hasil di bawah, kita mengetahui bahwa tidak ada missing value pada data dan data dapat dianalisis lebih lanjut.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># cek missing value</span>
<span style="color:#a6e22e">anyNA</span>(USArrests)
</code></pre></div><pre><code>#&gt; [1] FALSE
</code></pre><p>Selanjutnya, kita dapat melakukan perhitungan (dis)similarity data. Namun, perhitungan (dis)similarity / jarak amat sensitif terhadap perbedaan skala antar variable data. Oleh karena itu, kita perlu melakukan standarisasi data terlebih dahulu agar skala pada tiap variable serupa. Pada data <code>USArrest</code> setiap variabel belum memiliki skala yang sama sehingga perlu dilakukan standarisasi.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># cek skala tiap variable data</span>
<span style="color:#a6e22e">summary</span>(USArrests)
</code></pre></div><pre><code>#&gt;      Murder          Assault         UrbanPop          Rape      
#&gt;  Min.   : 0.800   Min.   : 45.0   Min.   :32.00   Min.   : 7.30  
#&gt;  1st Qu.: 4.075   1st Qu.:109.0   1st Qu.:54.50   1st Qu.:15.07  
#&gt;  Median : 7.250   Median :159.0   Median :66.00   Median :20.10  
#&gt;  Mean   : 7.788   Mean   :170.8   Mean   :65.54   Mean   :21.23  
#&gt;  3rd Qu.:11.250   3rd Qu.:249.0   3rd Qu.:77.75   3rd Qu.:26.18  
#&gt;  Max.   :17.400   Max.   :337.0   Max.   :91.00   Max.   :46.00
</code></pre><p>Dalam melakukan standarisasi data, kita bisa menggunakan <strong>z-score standarization</strong>. Formula z-score standarization adalah sebagai berikut:</p>
<p><code>$$Z-Score = \frac{x - \mu}{\sigma}$$</code></p>
<p>di mana:</p>
<ul>
<li><code>x</code>: observasi data</li>
<li><code>\(\mu\)</code>: rata-rata variabel</li>
<li><code>\(\sigma\)</code>: standar deviasi pada variabel tersebut</li>
</ul>
<p>Pada R, kita bisa melakukan z-score standarization menggunakan fungsi <code>scale()</code>. Hasil standarisasi data akan kita simpan dalam objek <code>us_z</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># z-score standardization</span>
us_z <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">scale</span>(USArrests) 
<span style="color:#a6e22e">head</span>(us_z)
</code></pre></div><pre><code>#&gt;                Murder   Assault   UrbanPop         Rape
#&gt; Alabama    1.24256408 0.7828393 -0.5209066 -0.003416473
#&gt; Alaska     0.50786248 1.1068225 -1.2117642  2.484202941
#&gt; Arizona    0.07163341 1.4788032  0.9989801  1.042878388
#&gt; Arkansas   0.23234938 0.2308680 -1.0735927 -0.184916602
#&gt; California 0.27826823 1.2628144  1.7589234  2.067820292
#&gt; Colorado   0.02571456 0.3988593  0.8608085  1.864967207
</code></pre><h2 id="mengukur-dissimilarity">Mengukur (Dis)similarity</h2>
<p>Setelah membersihkan dan menyiapkan data, selanjutnya kita dapat melakukan <strong>perhitungan (dis)similarity data</strong>. Metode yang akan digunakan pada kasus ini yaitu <em>euclidean distance</em>, karena kita akan menjadikan besaran nilai pada tiap variable data sebagai karakteristik yang membedakan tiap observasinya (negara bagian).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># menghitung distance</span>
us_dist <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">dist</span>(x <span style="color:#f92672">=</span> us_z, method <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;euclidean&#34;</span>)
</code></pre></div><p>Fungsi <code>dist()</code> secara otomatis menghitung (dis)similarity antar observasi data. Objek hasil <code>dist()</code> tersebut nantinya dapat langsung dimasukkan kedalam fungsi clustering dan otomatis diolah sebagai <strong>distance matrix</strong>, namun berikut kurang lebih tampilannya dalam bentuk distance matrix.</p>
<pre><code>#&gt;             Alabama   Alaska  Arizona Arkansas California
#&gt; Alabama    0.000000 2.703754 2.293520 1.289810   3.263110
#&gt; Alaska     2.703754 0.000000 2.700643 2.826039   3.012541
#&gt; Arizona    2.293520 2.700643 0.000000 2.717758   1.310484
#&gt; Arkansas   1.289810 2.826039 2.717758 0.000000   3.763641
#&gt; California 3.263110 3.012541 1.310484 3.763641   0.000000
</code></pre><h2 class="tabset" id="modeling">Modeling</h2>
<p>Setelah persiapan data selesai, maka saatnya membuat clustering. Pembuatan clustering yang akan dicoba adalah 5 metode AGNES clustering yang telah dijelaskan pada bab <strong>&ldquo;Linkage Method&rdquo;</strong> sebelumnya. Hal ini bertujuan untuk membandingkan rupa dendrogram yang dihasilkan oleh tiap metode.</p>
<p>Pembuatan AGNES clustering pada R dapat menggunakan fungsi <code>hclust()</code>. Fungsi ini akan meminta 2 argumen untuk dimasukkan yaitu:</p>
<ul>
<li><code>d</code>: objek distance / distance matrix</li>
<li><code>method</code>: linkage method yang digunakan, dengan pilihannya diantaranya:
<ul>
<li>&ldquo;ward.D2&rdquo;</li>
<li>&ldquo;single&rdquo;</li>
<li>&ldquo;complete&rdquo;</li>
<li>&ldquo;average&rdquo;</li>
<li>&ldquo;centroid&rdquo;</li>
</ul>
</li>
</ul>
<p>Selain linkage method yang kita bahas di atas, terdapat juga linkage method yang lain. Dokumentasi lebih lengkap mengenai linkage method yang disediakan <code>hclust()</code> ada <a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/hclust">disini</a>, dan bacaan lebih lanjut terkait penjelasan linkage method tersebut dapat dilihat <a href="https://stats.stackexchange.com/questions/195446/choosing-the-right-linkage-method-for-hierarchical-clustering">disini</a>.</p>
<ul>
<li><strong>Complete Linkage</strong></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">us_hc_complete <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">hclust</span>(d <span style="color:#f92672">=</span> us_dist, method <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;complete&#34;</span>)
</code></pre></div><ul>
<li><strong>Single Linkage</strong></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">us_hc_single <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">hclust</span>(d <span style="color:#f92672">=</span> us_dist, method <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;single&#34;</span>)
</code></pre></div><ul>
<li><strong>Average Linkage</strong></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">us_hc_avg <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">hclust</span>(d <span style="color:#f92672">=</span> us_dist, method <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;average&#34;</span>)
</code></pre></div><ul>
<li><strong>Centroid Linkage</strong></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">set.seed</span>(<span style="color:#ae81ff">100</span>)
us_hc_centroid <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">hclust</span>(d <span style="color:#f92672">=</span> us_dist, method <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;centroid&#34;</span>)
</code></pre></div><ul>
<li><strong>Ward&rsquo;s Minimum Variance</strong></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">us_hc_ward <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">hclust</span>(d <span style="color:#f92672">=</span> us_dist, method <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;ward.D2&#34;</span>)
</code></pre></div><p>Pada setiap metode clustering tersebut akan membentuk sebuah dendrogram. Dendrogram dapat divisualisasikan menggunakan fungsi <code>plot()</code> biasa atau bisa menggunakan fungsi <code>fviz_dend()</code> dari package <strong>factoextra</strong>.</p>
<h3 id="dendrogram-complete-linkage">Dendrogram Complete Linkage</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">library</span>(factoextra)
<span style="color:#a6e22e">fviz_dend</span>(us_hc_complete, cex <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>, 
          main <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Cluster Dendrogram Complete Linkage&#34;</span>)
</code></pre></div><p><!-- raw HTML omitted --></p>
<p>Pada dendrogram tersebut, setiap observasi yang memiliki kemiripan akan dihubungkan oleh sebuah garis dan bersatu pada suatu titik. Panjang garis tersebut disebut juga <strong>cophenetic distance</strong> dan berbeda-beda untuk tiap observasi. Panjang garis tersebut merupakan &ldquo;Height&rdquo; (y-axis) yang menggambarkan (dis)similarity/jarak antar kedua observasi.</p>
<p>Hasil yang terbentuk dari dendrogram complete linkage menunjukkan dendrogram yang cukup <strong>compact</strong>, dimana setiap cluster terlihat memiliki <strong>partisi yang cukup jelas</strong>. Hal ini akan mempermudah saat pemotongan dendrogram untuk membentuk banyak cluster.</p>
<p>Untuk melakukan pengelompokan pada pada dendrogram, kita bisa melakukan pemotongan dendrogram pada nilai (dis)similarity tertentu untuk mendapatkan sejumlah cluster yang terbentuk. Untuk melakukan pemotongan dendrogram, R menyediakan fungsi <code>cutree()</code> dan kita dapat mendefinisikan <code>h</code> yaitu nilai (dis)similarity yang ingin dipakai, atau <code>k</code> yaitu banyaknya cluster yang ingin kita bentuk.</p>
<p>Pada metode complete linkage ini akan kita coba potong menjadi 4 cluster.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># cutree untuk menghasilkan cluster data</span>
complete_clust <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cutree</span>(us_hc_complete,  k <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>)
<span style="color:#a6e22e">head</span>(complete_clust)
</code></pre></div><pre><code>#&gt;    Alabama     Alaska    Arizona   Arkansas California   Colorado 
#&gt;          1          1          2          3          2          2
</code></pre><p>Berikut ini banyak anggota dari masing-masing cluster yang terbentuk.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">table</span>(complete_clust)
</code></pre></div><pre><code>#&gt; complete_clust
#&gt;  1  2  3  4 
#&gt;  8 11 21 10
</code></pre><p>Anggota pada masing-masing cluster cukup merata dengan cluster yang paling banyak memiliki anggota adalah cluster 3. Berikut hasil akhir cluster yang terbentuk apabila dilihat dari dendrogram.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">fviz_dend</span>(us_hc_complete, k <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>, k_colors <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;jco&#34;</span>, rect <span style="color:#f92672">=</span> T, 
          main <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Complete Linkage Cluster&#34;</span>)
</code></pre></div><p><!-- raw HTML omitted --></p>
<p>Cara membaca cluster pada dendrogram sebagai berikut :</p>
<ul>
<li>Cluster 1 digambarkan oleh dendrogram berwarna merah.</li>
<li>Cluster 2 digambarkan oleh dendrogram berwarna abu-abu.</li>
<li>Cluster 3 digambarkan oleh dendrogram berwarna kuning.</li>
<li>Cluster 4 digambarkan oleh dendrogram berwarna biru.</li>
</ul>
<p>Salah satu cara yang dapat digunakan untuk memvalidasi cluster tree yaitu dengan menghitung korelasi antara <strong>cophenetic distance</strong> dengan <strong>jarak original</strong> masing-masing observasi. Semakin kuat nilai korelasi jarak antar observasi, maka cluster tersebut merepresentasikan data yang dimiliki.</p>
<p>Untuk melihat korelasinya, maka kita perlu menghitung cophenetic distance terlebih dahulu menggunakan fungsi <code>cophenetic()</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">complete_coph <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cophenetic</span>(us_hc_complete)
<span style="color:#a6e22e">cor</span>(complete_coph, us_dist)
</code></pre></div><pre><code>#&gt; [1] 0.6979437
</code></pre><h3 id="dendrogram-single-linkage">Dendrogram Single Linkage</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">fviz_dend</span>(us_hc_single, cex <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>, 
          main <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Cluster Dendrogram Single Linkage&#34;</span>)
</code></pre></div><p><!-- raw HTML omitted --></p>
<p>Hasil dendrogram yang terbentuk pada model single linkage terlihat lebih lebar dimana antar anggota cluster yang berbeda pun terlihat memiliki Height / (dis)similarity yang rendah. Cluster yang terbut terlihat rapat dan sangat dekat sehingga <strong>partisi dari tiap cluster tidak tervisualisasikan dengan maksimal</strong>. Hal ini dikarenakan proses pembuatan hirarki dendrogram menggunakan minimum (dis)similarity pada setiap clusternya.</p>
<p>Sangat sulit untuk menentukan banyak cluster yang akan terbentuk apabila setiap observasi sangat rapat. Apabila dicoba untuk membentuk 4 cluster, maka berikut ini hasil cluster yang terbentuk.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">single_clust <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cutree</span>(us_hc_single, k <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>)
<span style="color:#a6e22e">head</span>(single_clust)
</code></pre></div><pre><code>#&gt;    Alabama     Alaska    Arizona   Arkansas California   Colorado 
#&gt;          1          2          1          1          3          1
</code></pre><p>Banyak anggota pada masing-masing cluster adalah sebagai berikut.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">table</span>(single_clust)
</code></pre></div><pre><code>#&gt; single_clust
#&gt;  1  2  3  4 
#&gt; 46  1  2  1
</code></pre><p>Anggota pada cluster yang terbentuk sangat berbeda secara signifikan. Cluster 1 memiliki anggota yang paling banyak hampir sebanyak total observasi pada data. Hal ini yang dikarenakan terlalu rapatnya antar observasi sehingga sulit dipisahkan perbedaannya.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">fviz_dend</span>(us_hc_single, k <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>, k_colors <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;jco&#34;</span>, rect <span style="color:#f92672">=</span> T, 
          main <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Single Linkage Cluster&#34;</span>)
</code></pre></div><p><!-- raw HTML omitted --></p>
<p>Cara membaca cluster pada dendrogram yaitu  sebagai berikut :</p>
<ul>
<li>Cluster 1 digambarkan oleh dendrogram berwarna merah.</li>
<li>Cluster 2 digambarkan oleh dendrogram berwarna biru.</li>
<li>Cluster 3 digambarkan oleh dendrogram berwarna abu-abu.</li>
<li>Cluster 4 digambarkan oleh dendrogram berwarna kuning.</li>
</ul>
<p>Apabila kita cek validasi dari cluster tree yang terbentuk, korelasi yang diperoleh rendah, artinya cluster yang terbentuk kurang merepresentasikan kondisi data yang ada. Sehingga menggunakan single linkage untuk data <code>USArrest</code> kurang relevan.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">single_coph <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cophenetic</span>(us_hc_single)
<span style="color:#a6e22e">cor</span>(single_coph, us_dist)
</code></pre></div><pre><code>#&gt; [1] 0.541272
</code></pre><h3 id="dendrogram-average-linkage">Dendrogram Average Linkage</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">fviz_dend</span>(us_hc_avg, cex <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>, main <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Cluster Dendrogram Average Linkage&#34;</span>)
</code></pre></div><p><!-- raw HTML omitted --></p>
<p>Hasil dendrogram yang terbentuk pada model average linkage terlihat hampir mirip seperti complete linkage, yaitu terlihat compact. Model average linkage biasa digunakan karena menghasilkan nilai validasi cluster tree yang lebih baik dibandingkan metode linkage yang lain.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">avg_coph <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cophenetic</span>(us_hc_avg)
<span style="color:#a6e22e">cor</span>(avg_coph, us_dist)
</code></pre></div><pre><code>#&gt; [1] 0.7180382
</code></pre><p>Apabila dendrogramnya dipotong untuk membentuk 4 cluster, berikut hasil yang diperoleh.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">avg_clust <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cutree</span>(us_hc_avg, k <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>)
<span style="color:#a6e22e">table</span>(avg_clust)
</code></pre></div><pre><code>#&gt; avg_clust
#&gt;  1  2  3  4 
#&gt;  7  1 12 30
</code></pre><p>Anggota cluster paling banyak diperoleh pada cluster 4 sebanyak 30 cluster. Namun terdapat cluster yang hanya memiliki 1 anggota saja yaitu pada cluster 2. Hal ini dapat dijadikan indikasi bahwa observasi pada cluster 2 yaitu &ldquo;Alaska&rdquo; merupakan anomali data.</p>
<p>Berikut visualisasi dendrogram yang terbentuk pada masing-masing cluster.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">fviz_dend</span>(us_hc_avg, k <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>, k_colors <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;jco&#34;</span>, rect <span style="color:#f92672">=</span> T, main <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Average Linkage Cluster&#34;</span>)
</code></pre></div><p><!-- raw HTML omitted --></p>
<p>Cara membaca cluster pada dendrogram yaitu  sebagai berikut :</p>
<ul>
<li>Cluster 1 digambarkan oleh dendrogram berwarna abu-abu.</li>
<li>Cluster 2 digambarkan oleh dendrogram berwarna kuning.</li>
<li>Cluster 3 digambarkan oleh dendrogram berwarna merah.</li>
<li>Cluster 4 digambarkan oleh dendrogram berwarna biru.</li>
</ul>
<h3 id="dendrogram-centroid-linkage">Dendrogram Centroid Linkage</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">fviz_dend</span>(us_hc_centroid, cex <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>, main <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Cluster Dendrogram Centroid Linkage&#34;</span>)
</code></pre></div><p><!-- raw HTML omitted --></p>
<p>Dendrogram yang terbentuk sangat berbeda dengan dedrogram pada model sebelumnya. Terdapat ketidak teraturan distance pada dendrogram di beberapa cabang, dimana nilai distance antar cluster yang lebih rendah dibandingkan nilai distance antar observasinya. Kondisi ini disebut juga <strong>inversion</strong>.</p>
<p>Hal ini dikarenakan pada centroid linkage terdapat pembentukan centroid atau pusat cluster yang &ldquo;memusatkan&rdquo; seluruh jarak observasi pada cluster. Sehingga ketika dihitung nilai (dis)similarity antar cluster, dapat dihasilkan nilai (dis)similarity yang lebih rendah dibandingkan (dis)similarity antar observasinya. Dalam kata lain, similarity dapat meningkat selama proses clustering. Hal inilah yang perlu diperhatikan bila menggunakan centroid method. Diskusi lebih lanjut dapat dilihat <a href="https://stats.stackexchange.com/questions/217519/centroid-linkage-clustering-with-hclust-yields-wrong-dendrogram">disini</a>.</p>
<p>Meskipun begitu, apabila dilihat dari nilai korelasi distancenya menunjukkan nilai yang sedikit cukup merepresentasikan data yang ada.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">centroid_coph <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cophenetic</span>(us_hc_centroid)
<span style="color:#a6e22e">cor</span>(centroid_coph, us_dist)
</code></pre></div><pre><code>#&gt; [1] 0.6074717
</code></pre><p>Berikut ini anggota cluster pada 4 cluster yang terbentuk.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">centroid_clust <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cutree</span>(us_hc_centroid, k <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>)
<span style="color:#a6e22e">table</span>(centroid_clust)
</code></pre></div><pre><code>#&gt; centroid_clust
#&gt;  1  2  3  4 
#&gt; 47  1  1  1
</code></pre><p>Anggota cluster didominasi pada cluster 1 sebanyak 47 observasi, namun untuk cluster yang lain hanya memiliki 1 observasi saja. Berikut visualiasi cluster yang terbentuk untuk masing-masing dendrogramnya.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">fviz_dend</span>(us_hc_centroid, k <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>, k_colors <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;jco&#34;</span>, rect <span style="color:#f92672">=</span> T, 
          main <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Centroid Linkage Cluster&#34;</span>)
</code></pre></div><p><!-- raw HTML omitted --></p>
<p>Hasil partisi cluster yang terbentuk tidak sesuai dengan yang seharusnya. Berikut deskripsi anggota cluster yang seharusnya:</p>
<ul>
<li>Cluster 1 terdiri dari negara New Jersey hingga California.</li>
<li>Cluster 2 beranggotakan North California.</li>
<li>Cluster 3 beranggotakan Alaska.</li>
<li>Cluster 4 beranggotakan Vermont.</li>
</ul>
<p>Sepertinya terdapat kesulitan dalam melakukan pemotongan otomatis pada dendrogram yang mungkin dikarenakan oleh adanya <em>inversion</em>.</p>
<h3 id="dendrogram-wards-minimum-variance">Dendrogram Ward&rsquo;s Minimum Variance</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">fviz_dend</span>(us_hc_ward, cex <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>, 
          main <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Cluster Dendrogram Ward&#39;s Minimum Variance&#34;</span>)
</code></pre></div><p><!-- raw HTML omitted --></p>
<p>Dendrogram yang diperoleh dari metode ward&rsquo;s minimum variance terlihat <strong>terpartisi dengan sangat baik</strong>. Pada tiap cluster, terlihat nilai height yang rendah atau similarity yang amat tinggi antar observasinya. Hal ini dikarenakan cara kerja metode ward&rsquo;s minimum variance yang meminimumkan nilai within sum of squared (wss) tiap cluster. Umumnya, dendrogram metode ini menghasilkan partisi yang baik dan dikatkan yang paling menyerupai hasil <em>k-means clustering</em> (salah satu metode partitional clustering).</p>
<p>Berdasarkan nilai korelasi distance untuk metode ward&rsquo;s, diperoleh 0.69, yang mana dari segi nilai tidak terlalu besar, namun untuk melakukan partisi model cukup mudah.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">ward_coph <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cophenetic</span>(us_hc_ward)
<span style="color:#a6e22e">cor</span>(ward_coph, us_dist)
</code></pre></div><pre><code>#&gt; [1] 0.6975266
</code></pre><p>Berikut ini anggota cluster pada 4 cluster yang terbentuk.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">ward_clust <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cutree</span>(us_hc_ward, k <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>)
<span style="color:#a6e22e">table</span>(ward_clust)
</code></pre></div><pre><code>#&gt; ward_clust
#&gt;  1  2  3  4 
#&gt;  7 12 19 12
</code></pre><p>Anggota cluster yang diperoleh pada model ward&rsquo;s cukup mirip seperti model complete linkage. Anggota cluster yang paling sedikit yaitu pada cluster 1.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">fviz_dend</span>(us_hc_ward, k <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>, k_colors <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;jco&#34;</span>, rect <span style="color:#f92672">=</span> T, 
          main <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Ward&#39;s Minimum Variance Cluster&#34;</span>)
</code></pre></div><p><!-- raw HTML omitted --></p>
<p>Cara membaca cluster pada dendrogram sebagai berikut :</p>
<ul>
<li>Cluster 1 digambarkan oleh dendrogram berwarna biru</li>
<li>Cluster 2 digambarkan oleh dendrogram berwarna kuning</li>
<li>Cluster 3 digambarkan oleh dendrogram berwarna merah</li>
<li>Cluster 4 digambarkan oleh dendrogram berwarna abu-abu</li>
</ul>
<h3 id="dendrogram-terpilih">Dendrogram Terpilih</h3>
<p>Dari kelima dendrogram yang sudah dibuat, kita dapat memilih salah satu yang ingin digunakan. Pemilihannya dapat berdasarkan hasil validasi cluster tree menggunakan korelasi <strong>cophenetic distance</strong> dengan <strong>jarak original</strong> masing-masing observasi yang tadi telah dihitung:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">data.frame</span>(complete <span style="color:#f92672">=</span> <span style="color:#a6e22e">cor</span>(complete_coph, us_dist),
          single <span style="color:#f92672">=</span> <span style="color:#a6e22e">cor</span>(single_coph, us_dist),
          average <span style="color:#f92672">=</span> <span style="color:#a6e22e">cor</span>(avg_coph, us_dist),
          centroid <span style="color:#f92672">=</span> <span style="color:#a6e22e">cor</span>(centroid_coph, us_dist),
          ward <span style="color:#f92672">=</span> <span style="color:#a6e22e">cor</span>(ward_coph, us_dist)) <span style="color:#f92672">%&gt;%</span> 
  tidyr<span style="color:#f92672">::</span><span style="color:#a6e22e">pivot_longer</span>(cols <span style="color:#f92672">=</span> <span style="color:#a6e22e">colnames</span>(.),names_to <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;method&#34;</span>, values_to <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;correlation&#34;</span>)
</code></pre></div><pre><code>#&gt; # A tibble: 5 x 2
#&gt;   method   correlation
#&gt;   &lt;chr&gt;          &lt;dbl&gt;
#&gt; 1 complete       0.698
#&gt; 2 single         0.541
#&gt; 3 average        0.718
#&gt; 4 centroid       0.607
#&gt; 5 ward           0.698
</code></pre><p>Dapat terlihat bahwa nilai korelasi yang dihasilkan <strong>average linkage</strong> paling tinggi dibanding metode lainnya, disusul oleh <strong>complete linkage</strong>, dan <strong>ward linkage</strong>. Mempertimbangkan pula dendrogram yang dihasilkan, dendrogram dari average linkage menghasilkan cluster outlier yang hanya berisi negara bagian Alaska, dan ditakutkan membuat hasil clustering tidak optimal. Oleh karena itu, akan dipilih metode <strong>complete linkage</strong> dengan pertimbangan hasil correlation yang tinggi dan menghasilkan dendrogram yang cukup mudah diinterpretasikan.</p>
<h2 id="validasi-cluster">Validasi Cluster</h2>
<p>Dalam hierarchical clustering, menentukan banyaknya cluster optimal yang harus dibentuk terbilang cukup sulit. Secara umum, untuk menentukan cluster yang baik terdapat dua pengukuran yang sering dilakukan:</p>
<ol>
<li><strong>Internal measures</strong></li>
</ol>
<p>Internal measures merupakan pengukuran untuk melakukan cluster validation berdasarkan informasi internal yang terdapat pada data. Metode yang biasa digunakan pada internal measure yaitu <strong>connectivity</strong>, <strong>dunn index</strong>, dan <strong>silhouette</strong>. Nilai connectivity berkisar antara 0-inf, dimana cluster yang baik memiliki connectivity yang kecil. Sementara itu, nilai silhouette dan dunn index diharapkan tinggi untuk mengatakan bahwa clustering semakin baik.</p>
<ol start="2">
<li><strong>Stability measures</strong></li>
</ol>
<p>Stability measures merupakan pengukuran validation cluster yang lebih mendalam dibandingkan internal measures. Stability mengukur tingkat konsistensi dari hasil cluster yang terbentuk apabila terdapat satu variabel yang dihilangkan dalam satu waktu. Pengukuran yang dilakukan pada stability yaitu:</p>
<ul>
<li><strong>Average proportion of non-overlap (APN)</strong>: mengukur proporsi &ldquo;rata-rata jumlah observasi pada cluster yang sama&rdquo; antara dendrogram yang dibuat menggunakan data yang lengkap dan data yang sudah dihilangkan satu variabelnya.</li>
<li><strong>Average distance (AD)</strong>: jarak rata-rata antar observasi dalam cluster yang sama, pada pembuatan cluster dengan data yang lengkap dan data yang sudah dihilangkan satu variabelnya.</li>
<li><strong>Average distance between means (ADM)</strong>: jarak rata-rata antar pusat cluster pada pembuatan cluster dengan data yang lengkap dan data yang sudah dihilangkan satu variabelnya.</li>
<li><strong>Figure of merit (FOM)</strong>: rata-rata variansi intra-cluster pada variabel yang dihapus, dimana clustering dilakukan menggunakan sisa kolom yang tidak terhapus.</li>
</ul>
<blockquote>
<p>Nilai dari APN, ADM, dan FOM measure stability diatas memiliki range 0-1, di mana semakin kecil nilainya, maka akan semakin konsisten hasil clustering yang terbentuk. Sedangkan nilai AD berkisar pada range 0-inf, semakin kecil nilainya juga semakin baik hasil clusternya[^6].
Bahasan lebih jauh mengenai cluster validation dapat dibaca <a href="https://www.datanovia.com/en/lessons/cluster-validation-statistics-must-know-methods/">disini</a>.</p>
</blockquote>
<p>Untuk melakukan validasi cluster, R menyediakan package <code>clValid</code> dengan fungsi yang digunakan adalah <code>clValid()</code>. Kita akan coba cek jumlah cluster optimal berdasarkan internal measures dan stability measures menggunakan data <code>us_z</code> yang sudah dilakukan standarisasi.</p>
<p>Fungsi <code>clValid()</code> memiliki beberapa argumen diantaranya:</p>
<ul>
<li><code>obj</code>: data yang akan digunakan dalam clustering</li>
<li><code>nClust</code>: banyak cluster yang akan di evaluasi</li>
<li><code>clMethods</code>: metode clustering yang digunakan, beberapa diantaranya yang bisa digunakan yaitu &ldquo;hierarchical, &ldquo;kmeans&rdquo;, &ldquo;diana&rdquo;, &ldquo;fanny&rdquo;, &ldquo;som&rdquo;, &ldquo;model&rdquo;, &ldquo;sota&rdquo;, &ldquo;pam&rdquo;, &ldquo;clara&rdquo;, dan &ldquo;agnes&rdquo;. Bisa menggunakan lebih dari satu metode yang akan di evaluasi.</li>
<li><code>validation</code>: pengukuran validasi cluster yang digunakan, beberapa diantaranya yaitu &ldquo;internal&rdquo;, &ldquo;stability&rdquo;, dan &ldquo;biological&rdquo;. Bisa menggunakan lebih dari satu metode yang akan di evaluasi.</li>
<li><code>metric</code>: pengukuran (dis)similarity yang digunakan, beberapa yang bisa digunakan yaitu &ldquo;euclidean&rdquo;, &ldquo;correlation&rdquo;, dan &ldquo;manhattan&rdquo;.</li>
<li><code>method</code>: metode hierarchical clustering yang digunakan, beberapa yang bisa digunakan yaitu &ldquo;ward&rdquo;, &ldquo;single&rdquo;, &ldquo;complete&rdquo;, dan &ldquo;average&rdquo;.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">library</span>(clValid)
<span style="color:#75715e"># internal measures</span>
internal <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">clValid</span>(us_z, nClust <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span><span style="color:#f92672">:</span><span style="color:#ae81ff">5</span>, 
                    clMethods <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;agnes&#34;</span>, 
                    validation <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;internal&#34;</span>, 
                    metric <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;euclidean&#34;</span>,
                    method <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;complete&#34;</span>)
<span style="color:#a6e22e">summary</span>(internal)
</code></pre></div><pre><code>#&gt; 
#&gt; Clustering Methods:
#&gt;  agnes 
#&gt; 
#&gt; Cluster sizes:
#&gt;  3 4 5 
#&gt; 
#&gt; Validation Measures:
#&gt;                           3       4       5
#&gt;                                            
#&gt; agnes Connectivity  11.8730 24.7817 25.1067
#&gt;       Dunn           0.2648  0.1622  0.1709
#&gt;       Silhouette     0.3692  0.3160  0.3174
#&gt; 
#&gt; Optimal Scores:
#&gt; 
#&gt;              Score   Method Clusters
#&gt; Connectivity 11.8730 agnes  3       
#&gt; Dunn          0.2648 agnes  3       
#&gt; Silhouette    0.3692 agnes  3
</code></pre><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># stabilitas measures</span>
stability <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">clValid</span>(us_z, nClust <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span><span style="color:#f92672">:</span><span style="color:#ae81ff">4</span>, 
                     clMethods <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;agnes&#34;</span>, 
                     validation <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;stability&#34;</span>, 
                     metric <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;euclidean&#34;</span>,
                     method <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;complete&#34;</span>)
<span style="color:#75715e"># hanya menampilkan skor optimal</span>
<span style="color:#a6e22e">optimalScores</span>(stability)
</code></pre></div><pre><code>#&gt;         Score Method Clusters
#&gt; APN 0.2311732  agnes        4
#&gt; AD  1.6246247  agnes        4
#&gt; ADM 0.5947000  agnes        4
#&gt; FOM 0.7425723  agnes        4
</code></pre><p>Berdasarkan hasil validasi cluster, internal measures menghasilkan jumlah cluster terbaik di angka 3, sementara stability measures menghasilkan angka 4. <strong>Penggunaan cluster sebanyak 3 atau 4 bisa dijadikan opsi untuk melakukan partisi data</strong>. Nilai yang lebih baik dapat ditentukan berdasarkan kemudahan pengguna untuk melakukan interpretasi (misalnya profiling/analisis karakteristik) untuk tiap clusternya.</p>
<h2 id="kesimpulan">Kesimpulan</h2>
<p>Kita telah melakukan tahap demi tahap pembuatan hierarchical clustering dari pembacaan data hingga pembentukan dendrogram, berikut pemotongan dendrogramnya untuk clustering data. Berikut adalah tampilan dendrogram terpilih menggunakan pemotongan 4 cluster dan beberapa insight yang dapat diambil.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">fviz_dend</span>(us_hc_complete, k <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>, k_colors <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;jco&#34;</span>, rect <span style="color:#f92672">=</span> T, 
          main <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Complete Linkage Cluster&#34;</span>)
</code></pre></div><p><!-- raw HTML omitted --></p>
<p>Terdapat 2 cluster besar dan tiap cluster besar dapat dipecah kembali menjadi 2 cluster yang lebih kecil. Kemiripan antara negara bagian berdasarkan tingkat kriminalitasnya dapat terlihat, dan pasangan negara bagian yang paling serupa adalah Iowa &amp; New Hampshire.</p>
<h1 id="aplikasi-hierarchical-clustering">Aplikasi Hierarchical Clustering</h1>
<p>Hierarchical Clustering memiliki keunggulan yang tidak dimiliki oleh metode clustering lain, yaitu dapat merepresentasikan kedekatan antar tiap data dengan dendrogramnya. Hal ini amat bermanfaat khususnya dalam <strong>analisis network/komunitas</strong> atau deteksi data dari suatu komunitas. Oleh karena itu hierarchical clustering banyak dilakukan pada kasus-kasus berikut:</p>
<ol>
<li><a href="https://www.hindawi.com/journals/complexity/2017/3719428/">Social Network Community Detection</a></li>
<li><a href="https://www.floridamuseum.ufl.edu/science/at-last-butterflies-get-a-bigger-better-evolutionary-tree/">Analisis Evolusi Mahluk Hidup - Biodiversity of Butterflies</a></li>
<li><a href="https://academic.oup.com/bioinformatics/article/34/23/4121/5001388">Analisis Penyebaran Penyakit - Nextrain for COVID19 Tracking</a></li>
</ol>
<h1 id="referensi">Referensi</h1>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Tan, P.N., Steinbach, M., Kumar, V. (2006) Introduction to Data Mining. Boston: Pearson Education.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Fred, A.L.N. dan Leitao, J.M.N. (2000) Partitional vs Hierarchical Clustering Using a Minimum Grammar Complexity Approach, di F.J. Ferri et al. (Eds.): SSPR&amp;SPR 2000, LNCS 1876, hal. 193-202. Berlin: Springer-Verlag&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>University of Cincinnati Business Analytics. UC Business Analytics R Programming Guide, Bab <a href="https://uc-r.github.io/hc_clustering">Hierarchical Clustering</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p><a href="https://www.datanovia.com/en/lessons/clustering-distance-measures/">(Dis)similarity Measure</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>Rhys, H.I. (2020) <a href="https://livebook.manning.com/book/machine-learning-for-mortals-mere-and-otherwise/chapter-17/">Machine Learning with R, the tidyverse, and mlr</a>. USA: Manning Publications Co.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

  </div>

  <footer>
    <ul class="stats">
  <li class="categories">
    <ul>
        
            
            
                <i class="fa fa-folder"></i>
                
                
                <li><a class="article-category-link" href="https://algotech.netlify.com/categories/r">R</a></li>
                
            
        
    </ul>
  </li>
  <li class="tags">
    <ul>
        
            
            
                <i class="fa fa-tags"></i>
                
                
                <li><a class="article-category-link" href="https://algotech.netlify.com/tags/machine-learning">Machine Learning</a></li>
                
                
                <li><a class="article-category-link" href="https://algotech.netlify.com/tags/unsupervised-learning">Unsupervised Learning</a></li>
                
                
                <li><a class="article-category-link" href="https://algotech.netlify.com/tags/hierarchical-clustering">Hierarchical Clustering</a></li>
                
            
        
    </ul>
  </li>
</ul>

  </footer>

</article>

    <article class="post">
        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "https-algotech-netlify-com" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </article>


<ul class="actions pagination">
    
        <li><a href="https://algotech.netlify.com/blog/algoritma-optics/"
                class="button big previous">Algoritma OPTICS (Ordering Points to Identify the Clustering Structure)</a></li>
    

    
        <li><a href="https://algotech.netlify.com/blog/rplicate-series-bold-axis-character-with-ggplot2/"
                class="button big next">Rplicate Series: Bold Axis &amp; Character with ggplot2</a></li>
    
</ul>


    </div>
    
<section id="sidebar">

  
  <section id="intro">
    
    
      
        <a href='https://algotech.netlify.com/'><img src="https://algotech.netlify.com/img/main/logo.png" class="intro-circle" width="30%" alt="Hugo Future Imperfect" /></a>
      
    
    
      <header>
        <h2>Algoritma Technical Blog</h2>
        <p>We're a group of people who teach data science to individuals, trains companies and their employees to better profit from data. We care about the development of data science and a sense of community that connects our alumni and team with one another. To learn more about our approach to data science problems, feel free to hop over to our blog.</p>
      </header>
    
    
      <ul class="icons">
        
        
  <li><a href="//github.com/teamalgoritma" target="_blank" title="GitHub" class="fa fa-github"></a></li>



























  <li><a href="//linkedin.com/company/teamalgoritma" target="_blank" title="LinkedIn Company" class="fa fa-linkedin"></a></li>









  <li><a href="//facebook.com/teamalgoritma" target="_blank" title="Facebook" class="fa fa-facebook"></a></li>





















  <li><a href="//instagram.com/teamalgoritma" target="_blank" title="Instagram" class="fa fa-instagram"></a></li>





  <li><a href="//twitter.com/teamalgoritma" target="_blank" title="Twitter" class="fa fa-twitter"></a></li>




















      </ul>
    
  </section>

  
  <section class="recent-posts">
    <div class="mini-posts">
      <header>
        <h3>Upcoming Workshop</h3>
      </header>
      <div class="posts-container">
          <article class="mini-post">
            <header>
              
              
              <h3>
                <a href="https://algorit.ma/ds-course/dss-trading/?utm_source=algotech"><img src="https://algotech.netlify.com/img/2021/ads/dss_okt.png", alt="Building Algorithmic Trading Dashboard",  width="270" height="340">More details</a>
              </h3>
              
              <h3>
                <a href="https://algorit.ma/dds-oil-gas/?utm_source=algotech"><img src="https://algotech.netlify.com/img/2021/ads/dds-oilgas.png", alt="DDS: Secrets to a Successful Analytics Project in the Oil and Gas Industry",  width="270" height="340">More details</a>
              </h3>
            </header>
          </article>
      </div>

      
      
    </div>
  </section>

  
  
  

  
  

  
  <section id="footer">
    <p class="copyright">
      
        &copy; 2021
        
          Algoritma Technical Blog
        
      .
      Powered by <a href="//gohugo.io" target="_blank">Hugo</a>
    </p>
  </section>
</section>

    </div>
    <a id="back-to-top" href="#" class="fa fa-arrow-up fa-border fa-2x"></a>
    <style>
      .footer {
        position: fixed;
        left: 0;
        bottom: 0;
        width: 100%;
        height:50px; 
        background-color: black;
        color: white;
        text-align: center;
        padding-top: 15px;
        padding-bottom: 15px;
        padding-left: 50px;
        padding-right: 50px;
}
      }


      </style>

      <div class="footer">
        <p>
          Want to know more about our workshop?
          <a href="https://algorit.ma/?utm_source=algotech&utm_medium=content&utm_campaign=introduction-to-hierarchical-clustering" style="color: rgb(197, 38, 38)"> Visit our main website here</a> <br>
        </p>
          
      </div>
    

    
      
    

    
      
      
      
        <script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>
        
        
        
        <script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
        <script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
        <script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/css.min.js"></script>
        <script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>
      
    
    
    
      <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/skel/3.0.1/skel.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.1.25/jquery.fancybox.min.js"></script>
      <script src="https://algotech.netlify.com/js/util.js"></script>
      <script src="https://algotech.netlify.com/js/main.js"></script>
      <script src="https://algotech.netlify.com/js/backToTop.js"></script>
    

    
      
        
      
        
          <script src="https://algotech.netlify.com/js/bootstrap.min.js"></script>
        
      
    

    
    <script>hljs.initHighlightingOnLoad();</script>
      <script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


  </body>
</html>

