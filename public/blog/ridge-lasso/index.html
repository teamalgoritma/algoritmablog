<!DOCTYPE HTML>

<html>
    <head>
        <script type="application/ld+json">
    {
        "@context" : "http://schema.org",
        "@type" : "BlogPosting",
        "mainEntityOfPage": {
             "@type": "WebPage",
             "@id": "\/"
        },
        "articleSection" : "blog",
        "name" : "Ridge and LASSO Regression",
        "headline" : "Ridge and LASSO Regression",
        "description" : "Overview Regression analysis is a way that can be used to determine the relationship between the predictor variable (x) and the target variable (y).\nOrdinary Least Squares (OLS) is the most common estimation method for linear models and it applies for good reasons. As long as your model meets the OLS assumptions for linear regression, you can rest easy knowing that you get the best estimate.\nBut in the real world to meet OLS regression assumptions will be very difficult.",
        "inLanguage" : "en",
        "author" : "",
        "creator" : "",
        "publisher": "",
        "accountablePerson" : "",
        "copyrightHolder" : "",
        "copyrightYear" : "2019",
        "datePublished": "2019-12-18 00:00:00 \x2b0000 UTC",
        "dateModified" : "2019-12-18 00:00:00 \x2b0000 UTC",
        "url" : "\/blog\/ridge-lasso\/",
        "wordCount" : "3718",
        "keywords" : [ "regression","Machine Learning","overfitting","Regularization","Blog" ]
    }
    </script>
        
            
                <title>Ridge and LASSO Regression</title>
            
        

        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <meta name="generator" content="Hugo 0.60.1" />
        


        
            <meta name="author" content="Hafizah Ilma">
        
        
            
                <meta name="description" content="HTML5 UP theme, Future Imperfect with some extra goodies, ported by Julio Pescador. Powered by Hugo">
            
        

        <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Ridge and LASSO Regression"/>
<meta name="twitter:description" content="Overview Regression analysis is a way that can be used to determine the relationship between the predictor variable (x) and the target variable (y).
Ordinary Least Squares (OLS) is the most common estimation method for linear models and it applies for good reasons. As long as your model meets the OLS assumptions for linear regression, you can rest easy knowing that you get the best estimate.
But in the real world to meet OLS regression assumptions will be very difficult."/>
<meta name="twitter:site" content="@teamalgoritma"/>

        <meta property="og:title" content="Ridge and LASSO Regression" />
<meta property="og:description" content="Overview Regression analysis is a way that can be used to determine the relationship between the predictor variable (x) and the target variable (y).
Ordinary Least Squares (OLS) is the most common estimation method for linear models and it applies for good reasons. As long as your model meets the OLS assumptions for linear regression, you can rest easy knowing that you get the best estimate.
But in the real world to meet OLS regression assumptions will be very difficult." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/blog/ridge-lasso/" />
<meta property="article:published_time" content="2019-12-18T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-12-18T00:00:00+00:00" />

        <meta property="og:image" content="//images/logo.png">
        <meta property="og:image:type" content="image/png">
        <meta property="og:image:width" content="512">
        <meta property="og:image:height" content="512">
        <meta itemprop="name" content="Ridge and LASSO Regression">
<meta itemprop="description" content="Overview Regression analysis is a way that can be used to determine the relationship between the predictor variable (x) and the target variable (y).
Ordinary Least Squares (OLS) is the most common estimation method for linear models and it applies for good reasons. As long as your model meets the OLS assumptions for linear regression, you can rest easy knowing that you get the best estimate.
But in the real world to meet OLS regression assumptions will be very difficult.">
<meta itemprop="datePublished" content="2019-12-18T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2019-12-18T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="3718">



<meta itemprop="keywords" content="regression,Machine Learning,overfitting,Regularization," />
        

        
            
        

        
        
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">
            <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway:400,800,900|Source+Sans+Pro:400,700">
            <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.1.25/jquery.fancybox.min.css">
            <link rel="stylesheet" href="/css/main.css">
            <link rel="stylesheet" href="/css/add-on.css">
            <link rel="stylesheet" href="/css/academicons.min.css">
        

        
            
                
            
                
                    <link rel="stylesheet" href="/css/night-owl.css">
                
            
        


  
    
      <link rel="stylesheet" href="/css/night-owl.css" rel="stylesheet" id="theme-stylesheet">
      <script src="/js/highlight.pack.js"></script>
      <script>hljs.initHighlightingOnLoad();</script>
  


      





    </head>
    <body>

      
      <div id="wrapper">

    
    
<header id="header">
    
      <h1><a href="/">blog</a></h1>
    

    <nav class="links">
        <ul>
            
                <li>
                    <a href="/">
                            <i class="fa fa-home">&nbsp;</i>Home
                    </a>
                </li>
            
                <li>
                    <a href="/tags/machine-learning/">
                            <i class="fa fa-cog">&nbsp;</i>Machine Learning
                    </a>
                </li>
            
                <li>
                    <a href="/tags/data-visualization/">
                            <i class="fa fa-area-chart">&nbsp;</i>Data Visualization
                    </a>
                </li>
            
        </ul>
    </nav>
    <nav class="main">
        <ul>
            
            <li id="share-nav" class="share-menu" style="display:none;">
                <a class="fa-share-alt" href="#share-menu">Share</a>
            </li>
            
            <li class="search">
                <a class="fa-search" href="#search">Search</a>
                <form id="search" method="get" action="//google.com/search">
                    <input type="text" name="q" placeholder="Search" />
                    <input type="hidden" name="as_sitesearch" value="/">
                </form>
            </li>
            <li class="menu">
                <a class="fa-bars" href="#menu">Menu</a>
            </li>
        </ul>
    </nav>
</header>


<section id="menu">

    
        <section>
            <form class="search" method="get" action="//google.com/search">
                <input type="text" name="q" placeholder="Search" />
                <input type="hidden" name="as_sitesearch" value="/">
            </form>
        </section>

    
        <section>
            <ul class="links">
                
                    <li>
                        <a href="/">
                            <h3>
                                <i class="fa fa-home">&nbsp;</i>Home
                            </h3>
                        </a>
                    </li>
                
                    <li>
                        <a href="/tags/machine-learning/">
                            <h3>
                                <i class="fa fa-cog">&nbsp;</i>Machine Learning
                            </h3>
                        </a>
                    </li>
                
                    <li>
                        <a href="/tags/data-visualization/">
                            <h3>
                                <i class="fa fa-area-chart">&nbsp;</i>Data Visualization
                            </h3>
                        </a>
                    </li>
                
            </ul>
        </section>

    
        <section class="recent-posts">
            <div class="mini-posts">
                <header>
                    <h3>Recent Posts</h3>
                </header>
                

                
                    
                

                
                        <article class="mini-post">
                            <header>
                                <h3><a href="/blog/text-generation-with-markov-chains/">Text Generation with Markov Chains</a></h3>
                                
                                <time class="published" datetime=
                                    '2020-04-02'>
                                    April 2, 2020</time>
                            </header>
                            

                        </article>
                
                        <article class="mini-post">
                            <header>
                                <h3><a href="/blog/dbscan-clustering/">DBSCAN Clustering</a></h3>
                                
                                <time class="published" datetime=
                                    '2020-02-07'>
                                    February 7, 2020</time>
                            </header>
                            

                        </article>
                
                        <article class="mini-post">
                            <header>
                                <h3><a href="/blog/optimization-with-genetic-algorithm/">Optimization and Hyper-Parameter Tuning with Genetic Algorithm</a></h3>
                                
                                <time class="published" datetime=
                                    '2020-01-13'>
                                    January 13, 2020</time>
                            </header>
                            

                        </article>
                
                        <article class="mini-post">
                            <header>
                                <h3><a href="/blog/introduction-to-generative-adversarial-network-with-keras/">Introduction to Generative Adversarial Network with Keras</a></h3>
                                
                                <time class="published" datetime=
                                    '2019-12-18'>
                                    December 18, 2019</time>
                            </header>
                            

                        </article>
                
                        <article class="mini-post">
                            <header>
                                <h3><a href="/blog/ridge-lasso/">Ridge and LASSO Regression</a></h3>
                                
                                <time class="published" datetime=
                                    '2019-12-18'>
                                    December 18, 2019</time>
                            </header>
                            

                        </article>
                

                
                    <a href=
                        
                            /blog/
                        
                        class="button">View more posts</a>
                
            </div>
        </section>

    
        
</section>

    <section id="share-menu">
    <section id="social-share-nav">
        <ul class="links">
            <header>
                <h3>Share this post <i class="fa fa-smile-o"></i></h3>
            </header>
            



<li>
  <a href="https://twitter.com/intent/tweet?text=Ridge%20and%20LASSO%20Regression&amp;url=%2fblog%2fridge-lasso%2f" target="_blank" class="share-btn twitter">
    <i class="fa fa-twitter"></i>
    <p>Twitter</p>
    </a>
</li>








<li>
  <a href="https://www.facebook.com/sharer.php?u=%2fblog%2fridge-lasso%2f" target="_blank" class="share-btn facebook">
    <i class="fa fa-facebook"></i>
    <p>Facebook</p>
    </a>
</li>







<li>
  <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fblog%2fridge-lasso%2f&amp;title=Ridge%20and%20LASSO%20Regression" target="_blank" class="share-btn linkedin">
      <i class="fa fa-linkedin"></i>
      <p>LinkedIn</p>
    </a>
</li>











        </ul>
    </section>
</section>

    
    <div id="main">
        
        
        <article class="post">
  <header>
    <div class="title">
        
            <h2><a href="/blog/ridge-lasso/">Ridge and LASSO Regression</a></h2>
        
        
    </div>
    <div class="meta">
        

        <time class="published"
            datetime='2019-12-18'>
            December 18, 2019</time>
        <span class="author"><a href="">Hafizah Ilma</a></span>
        
            <p>18 minute read</p>
        
        
    </div>
</header>


  
    <section id="social-share">
      <ul class="icons">
        



<li>
  <a href="https://twitter.com/intent/tweet?text=Ridge%20and%20LASSO%20Regression&amp;url=%2fblog%2fridge-lasso%2f" target="_blank" class="share-btn twitter">
    <i class="fa fa-twitter"></i>
    <p>Twitter</p>
    </a>
</li>








<li>
  <a href="https://www.facebook.com/sharer.php?u=%2fblog%2fridge-lasso%2f" target="_blank" class="share-btn facebook">
    <i class="fa fa-facebook"></i>
    <p>Facebook</p>
    </a>
</li>







<li>
  <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fblog%2fridge-lasso%2f&amp;title=Ridge%20and%20LASSO%20Regression" target="_blank" class="share-btn linkedin">
      <i class="fa fa-linkedin"></i>
      <p>LinkedIn</p>
    </a>
</li>











      </ul>
    </section>
  

  

  <div id="content">
    <h2 id="overview">Overview</h2>
<p>Regression analysis is a way that can be used to determine the relationship between the predictor variable (x) and the target variable (y).</p>
<p>Ordinary Least Squares (OLS) is the most common estimation method for linear models and it applies for good reasons. As long as your model meets the OLS assumptions for linear regression, you can rest easy knowing that you get the best estimate.</p>
<p>But in the real world to meet OLS regression assumptions will be very difficult. Especially the assumption of &ldquo;Multicollinearity&rdquo;. Multicollinearity assumptions occur when the predictor variables are highly correlated with each other and there are many predictors. This is reflected in the formula for variance given above: if m approaches n, the variance approaches infinity. When multicollinearity occurs, the OLS estimator will <em>tend</em> to have very large variants, although with small bias. However, estimators that have very large variants will produce poor estimates. This phenomenon is referred to as <strong>Overfitting</strong>.</p>
<p>This graphic illustrates (Figure 1) what bias and variance are. Imagine the bull's-eye is the true population parameter that we are estimating, β, and the shots at it are the values of our estimates resulting from four different estimators</p>
<ul>
<li>
<p>low bias and variance,</p>
</li>
<li>
<p>high bias and variance,</p>
</li>
<li>
<p>low bias and high variance,</p>
</li>
<li>
<p>high bias and low variance.</p>
</li>
</ul>
<!-- raw HTML omitted -->
<p>Let’s say we have model which is very accurate, therefore the error of our model will be low, meaning a low bias and low variance as shown in first figure.  All the data points fit within the bulls-eye.</p>
<p>Now how this bias and variance is balanced to have a perfect model? Take a look at the image below and try to understand.</p>
<!-- raw HTML omitted -->
<p>In the picture above, it can be seen based on the complexity of the model that has 2 possible errors, namely:</p>
<ol>
<li>
<p>Underfitting</p>
</li>
<li>
<p>Overfitting</p>
</li>
</ol>
<p>To overcome <strong>Underfitting</strong> or high bias, we can basically add new parameters to our model so that the model complexity increases, and thus reducing high bias.</p>
<p>As a complexity model, which in the case of linear regression can be considered as the number of predictors, it increases, the expected variance also increases, but the bias decreases. An impartial OLS will place us on the right side of the image, which is far from optimal, this term is called <strong>Overfitting</strong>.</p>
<p>Now, how can we overcome Overfitting for a regression model?</p>
<p>Basically there are two methods to overcome overfitting,</p>
<p><strong>1. Reduce the model complexity</strong>,</p>
<p>To reduce the complexity of the model we can use stepwise Regression (forward or backward) selection for this, but that way we would not be able to tell anything about the removed variables&rsquo; effect on the response.</p>
<p>But we will introduce you to 2 Regularization Regression methods that are quite good and can be used to overcome overfitting obstacles.</p>
<p><strong>2. Regularization.</strong></p>
<h2 id="regularization">Regularization</h2>
<p>Regularization is a regression technique, which limits, regulates or shrinks the estimated coefficient towards zero. In other words, this technique does not encourage learning of more complex or flexible models, so as to avoid the risk of overfitting.</p>
<p>The formula for Multiple Linear Regression looks like this.</p>
<p><code>$$y = \beta_{0} + \beta_{i}x_{i}$$</code></p>
<p>Here Y represents the relationship studied and β represents the estimated coefficient for different variables or predictors (X).</p>
<p>The procedure for selecting a regression line uses an error value, known as Sum Square Error (SSE). Regression lines are formed when minimizing SSE values.</p>
<p>Where the SSE formula is as follows:</p>
<p><code>$$SSE =\sum_{x = 1}^{n} (y_{i}-\hat{y}_{i})^2$$</code></p>
<p>That's why if I want to predict house prices based on land area, but I only have 2 data train like Figure 3a below,</p>
<!-- raw HTML omitted -->
<p>then the regression model (line) that I have is like figure 3.</p>
<p>But the problem is what if it turns out I have test data and its distribution is like the blue dot below?</p>
<!-- raw HTML omitted -->
<p>Figure 4 above shows that if we have a regression line in orange, and want to predict test data like the blue dot above, the estimator has a very large variance even though it has a small bias value, and The regression model formed is very good for the data train but bad for the test data. When the phenomenon occurs, the model built is subject to <em>overfitting</em> problems.</p>
<p>There are two types of regression that are quite familiar and use this Regularization technique, namely:</p>
<ol>
<li>
<p>Ridge Regression</p>
</li>
<li>
<p>Lasso Regression</p>
</li>
</ol>
<h2 id="ridge-regression">Ridge Regression</h2>
<p>Ridge Regression is a variation of linear regression. We use ridge regression to tackle the multicollinearity problem. Due to multicollinearity, we see a very large variance in the least square estimates of the model. So to reduce this variance a degree of bias is added to the regression estimates.</p>
<p>Ordinary Least Square (OLS) will create a model by minimizing the value of Sum Square Error (SSE), Whereas The Rigde regression will create a model by minimizing :</p>
<p><code>$$SSE + λ \sum_{i = 1}^{n} (\beta_{i})^2$$</code></p>
<!-- raw HTML omitted -->
<p>It can be seen that the main idea of Ridge Regression is to add a little bias to reduce the value of the variance estimator.</p>
<p>It can be seen that the greater the value of λ (lambda) the regression line will be more horizontal, so the coefficient value approaches 0.</p>
<!-- raw HTML omitted -->
<p><strong>If λ = 0, the output is similar to simple linear regression.</strong></p>
<p><strong>If λ = very large , the coefficients value approaches 0.</strong></p>
<h2 id="lasso-regression">Lasso Regression</h2>
<p>LASSO (Least Absolute Shrinkage Selector Operator), The algorithm is another variation of linear regression like ridge regression. We use lasso regression when we have large number of predictor variables.</p>
<p>The equation of LASSO is similar to ridge regression and looks like as given below.</p>
<p><code>$$SSE + λ \sum_{i = 1}^{n} |\beta_{i}|$$</code></p>
<p>Here the objective is as follows: If λ = 0, We get same coefficients as linear regression If λ = vary large, All coefficients are shriked towards zero.</p>
<p><strong>The main difference between Ridge and LASSO Regression is that if ridge regression can shrink the coefficient close to 0 so that all predictor variables are retained. Whereas LASSO can shrink the coefficient to exactly 0 so that LASSO can select and discard the predictor variables that have the right coefficient of 0.</strong></p>
<h2 id="ordinary-least-square-ols-regression">Ordinary Least Square (OLS) Regression</h2>
<h3 id="libraries">Libraries</h3>
<p>The first thing to do is to prepare several libraries as below.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">library</span>(glmnet)  
<span style="color:#a6e22e">library</span>(caret)  
<span style="color:#a6e22e">library</span>(dplyr)   
<span style="color:#a6e22e">library</span>(car)
<span style="color:#a6e22e">library</span>(nnet)
<span style="color:#a6e22e">library</span>(GGally)
<span style="color:#a6e22e">library</span>(lmtest)
</code></pre></div><h3 id="read-data">Read Data</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">data</span>(mtcars)
mtcars <span style="color:#f92672">&lt;-</span> mtcars <span style="color:#f92672">%&gt;%</span> 
  <span style="color:#a6e22e">select</span>(<span style="color:#f92672">-</span>vs, <span style="color:#f92672">-</span>am)
<span style="color:#a6e22e">head</span>(mtcars)
</code></pre></div><pre><code>#&gt;                    mpg cyl disp  hp drat    wt  qsec gear carb
#&gt; Mazda RX4         21.0   6  160 110 3.90 2.620 16.46    4    4
#&gt; Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02    4    4
#&gt; Datsun 710        22.8   4  108  93 3.85 2.320 18.61    4    1
#&gt; Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44    3    1
#&gt; Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02    3    2
#&gt; Valiant           18.1   6  225 105 2.76 3.460 20.22    3    1
</code></pre><p>Now we will try to create an OLS model using <code>mtcars</code> data, which consists of 32 observations (rows) and 11 variables:</p>
<p>[, 1]	mpg	Miles/(US) gallon</p>
<p>[, 2]	cyl	Number of cylinders</p>
<p>[, 3]	disp	Displacement (cu.in.)</p>
<p>[, 4]	hp	Gross horsepower</p>
<p>[, 5]	drat	Rear axle ratio</p>
<p>[, 6]	wt	Weight (1000 lbs)</p>
<p>[, 7]	qsec	1/4 mile time</p>
<p>[, 8]	gear	Number of forward gears</p>
<p>[, 9]	carb	Number of carburetors</p>
<p>In this case, OLS Regression, Ridge Regression, and LASSO Regression will be applied to predict <em>mpg</em> based on 10 other predictor variables.</p>
<ul>
<li>Correlation</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">ggcorr</span>(mtcars, label <span style="color:#f92672">=</span> T)
</code></pre></div><p><!-- raw HTML omitted --></p>
<h3 id="data-partition">Data Partition</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">train <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">head</span>(mtcars,<span style="color:#ae81ff">24</span>)
test <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">tail</span>(mtcars,<span style="color:#ae81ff">8</span>)[,<span style="color:#ae81ff">-1</span>]

<span style="color:#75715e">## Split Data for Ridge and LASSO model</span>

xtrain <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">model.matrix</span>(mpg<span style="color:#f92672">~</span>., train)[,<span style="color:#ae81ff">-1</span>]
ytrain <span style="color:#f92672">&lt;-</span> train<span style="color:#f92672">$</span>mpg

test2 <span style="color:#f92672">&lt;-</span>  <span style="color:#a6e22e">tail</span>(mtcars,<span style="color:#ae81ff">8</span>)
ytest <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">tail</span>(mtcars,<span style="color:#ae81ff">8</span>)[,<span style="color:#ae81ff">1</span>]
xtest <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">model.matrix</span>(mpg<span style="color:#f92672">~</span>., test2)[,<span style="color:#ae81ff">-1</span>] 
</code></pre></div><h3 id="ols-regression-model">OLS Regression Model</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">ols <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">lm</span>(mpg<span style="color:#f92672">~</span>., train)
<span style="color:#a6e22e">summary</span>(ols)
</code></pre></div><pre><code>#&gt; 
#&gt; Call:
#&gt; lm(formula = mpg ~ ., data = train)
#&gt; 
#&gt; Residuals:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -4.3723 -1.1602 -0.1456  1.1948  4.1680 
#&gt; 
#&gt; Coefficients:
#&gt;             Estimate Std. Error t value Pr(&gt;|t|)  
#&gt; (Intercept) -5.42270   23.86133  -0.227   0.8233  
#&gt; cyl          0.69631    1.17331   0.593   0.5617  
#&gt; disp        -0.00103    0.01909  -0.054   0.9577  
#&gt; hp          -0.01034    0.03028  -0.342   0.7374  
#&gt; drat         5.36826    2.63249   2.039   0.0595 .
#&gt; wt          -0.47565    2.34992  -0.202   0.8423  
#&gt; qsec         0.01171    0.76250   0.015   0.9879  
#&gt; gear         3.44056    3.01636   1.141   0.2719  
#&gt; carb        -2.66168    1.25748  -2.117   0.0514 .
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 2.553 on 15 degrees of freedom
#&gt; Multiple R-squared:  0.8886,	Adjusted R-squared:  0.8293 
#&gt; F-statistic: 14.96 on 8 and 15 DF,  p-value: 0.000007346
</code></pre><h3 id="assumption-checking-of-ols-regression-model">Assumption Checking of OLS Regression Model</h3>
<p>Linear regression is an analysis that assesses whether one or more predictor variables explain the dependent (criterion) variable.  The regression has four key assumptions:</p>
<p><strong>1. Linearity</strong></p>
<p>linear regression needs the relationship between the predictor and target variables to be linear.  To test this linearity you can use the <code>cor.test ()</code> function.</p>
<ul>
<li>
<p>H0 : Correlation is not significant</p>
</li>
<li>
<p>H1 : Correlation is Significant</p>
</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">cbind</span>(<span style="color:#a6e22e">cor.test</span>(mtcars<span style="color:#f92672">$</span>mpg,mtcars<span style="color:#f92672">$</span>cyl)[[3]],
<span style="color:#a6e22e">cor.test</span>(mtcars<span style="color:#f92672">$</span>mpg,mtcars<span style="color:#f92672">$</span>disp)[[3]],
<span style="color:#a6e22e">cor.test</span>(mtcars<span style="color:#f92672">$</span>mpg,mtcars<span style="color:#f92672">$</span>hp)[[3]],
<span style="color:#a6e22e">cor.test</span>(mtcars<span style="color:#f92672">$</span>mpg,mtcars<span style="color:#f92672">$</span>drat)[[3]],
<span style="color:#a6e22e">cor.test</span>(mtcars<span style="color:#f92672">$</span>mpg,mtcars<span style="color:#f92672">$</span>wt)[[3]],
<span style="color:#a6e22e">cor.test</span>(mtcars<span style="color:#f92672">$</span>mpg,mtcars<span style="color:#f92672">$</span>qsec)[[3]],
<span style="color:#a6e22e">cor.test</span>(mtcars<span style="color:#f92672">$</span>mpg,mtcars<span style="color:#f92672">$</span>gear)[[3]],
<span style="color:#a6e22e">cor.test</span>(mtcars<span style="color:#f92672">$</span>mpg,mtcars<span style="color:#f92672">$</span>carb)[[3]])
</code></pre></div><pre><code>#&gt;                    [,1]               [,2]            [,3]         [,4]
#&gt; [1,] 0.0000000006112687 0.0000000009380327 0.0000001787835 0.0000177624
#&gt;                    [,5]       [,6]        [,7]        [,8]
#&gt; [1,] 0.0000000001293959 0.01708199 0.005400948 0.001084446
</code></pre><p>From the p-values above shows each predictor variable has a <strong>significant correlation</strong> on the target variable (mpg).</p>
<p><strong>2. Normalitas Residual</strong></p>
<p>OLS Regression requires residuals from a standard normal distribution model. Why? because if the residuals have a normal standard distribution, then the residual distribution is spread around with mean 0 and some variance.</p>
<ul>
<li>
<p>H0 : Residuals are normally distributed</p>
</li>
<li>
<p>H1 : Residuals are not normally distributed</p>
</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">shapiro.test</span>(ols<span style="color:#f92672">$</span>residuals)
</code></pre></div><pre><code>#&gt; 
#&gt; 	Shapiro-Wilk normality test
#&gt; 
#&gt; data:  ols$residuals
#&gt; W = 0.99087, p-value = 0.9979
</code></pre><p>Based on the above results obtained p-value (0.1709) &gt; alpha (0.05) so that it was concluded that Residuals are normally distributed.</p>
<p><strong>3. No Heteroskedasticity</strong></p>
<p>The assumption of No Heteroskedasticity or No Homoskedasticity means that the residual model has a homogeneous variant, and does not form a pattern. The No Heteroskedasticity assumption can be tested using the <code>bptest ()</code> function.</p>
<ul>
<li>
<p>H0 : Distributed residuals are homogenous</p>
</li>
<li>
<p>H1 : Distributed residuals are heterogenous</p>
</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">bptest</span>(ols)
</code></pre></div><pre><code>#&gt; 
#&gt; 	studentized Breusch-Pagan test
#&gt; 
#&gt; data:  ols
#&gt; BP = 15.888, df = 8, p-value = 0.044
</code></pre><p>Based on the above results obtained p-value (0.0356) &lt; alpha (0.05) so it can be concluded that resDistributed residuals are heterogenous</p>
<p><strong>4. No Multicolinearity</strong></p>
<p>Multicollinearity is a condition where there are at least 2 predictor variables that have a strong relationship. We expect that there is <strong>no multicollinearity</strong>. Multicollinearity is marked if the Variance Inflation Factor (VIF) value is &gt; 10.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">vif</span>(ols)
</code></pre></div><pre><code>#&gt;       cyl      disp        hp      drat        wt      qsec      gear 
#&gt; 14.649665 19.745238 11.505313  7.234090 18.973493  5.619677  8.146065 
#&gt;      carb 
#&gt;  8.646147
</code></pre><p>In the above output, there are 7 predictor variables that have a VIF value &gt; 10. This indicates that the estimator model of the ols model has a <strong>large variance (Overfitting Problem)</strong>.</p>
<h3 id="evidence-of-overfitting">Evidence of Overfitting</h3>
<p>Let's prove it by comparing the MSE (Mean Square Error) error value between the data train and the test data.</p>
<ul>
<li>Data Train</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">ols_pred <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">predict</span>(ols, newdata <span style="color:#f92672">=</span> train)
<span style="color:#a6e22e">mean</span>((ols_pred<span style="color:#f92672">-</span>ytrain)^2)
</code></pre></div><pre><code>#&gt; [1] 4.072081
</code></pre><ul>
<li>Data Test</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">ols_pred <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">predict</span>(ols, newdata <span style="color:#f92672">=</span> test)
<span style="color:#a6e22e">mean</span>((ols_pred<span style="color:#f92672">-</span>ytest)^2)
</code></pre></div><pre><code>#&gt; [1] 22.15188
</code></pre><p>As mentioned above, one way to overcome overfitting can be to reduce the dimensions using the Stepwise Regression method.</p>
<p>Stepwise regression is a combination of forwarding and backward methods, the first variable entered is the variable with the highest and significant correlation with the dependent variable, the second incoming variable is the variable with the highest and still significant correlation after certain variables enter the model then other variables that is in the model is evaluated, if there are variables that are not significant then the variable is excluded.</p>
<h2 id="stepwise-regression">Stepwise Regression</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">names</span>(mtcars)
</code></pre></div><pre><code>#&gt; [1] &quot;mpg&quot;  &quot;cyl&quot;  &quot;disp&quot; &quot;hp&quot;   &quot;drat&quot; &quot;wt&quot;   &quot;qsec&quot; &quot;gear&quot; &quot;carb&quot;
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">lm.all <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">lm</span>(mpg <span style="color:#f92672">~</span>., train)
stepwise_mod <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">step</span>(lm.all, direction<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">backward&#34;</span>)
</code></pre></div><pre><code>#&gt; Start:  AIC=51.7
#&gt; mpg ~ cyl + disp + hp + drat + wt + qsec + gear + carb
#&gt; 
#&gt;        Df Sum of Sq     RSS    AIC
#&gt; - qsec  1    0.0015  97.731 49.700
#&gt; - disp  1    0.0190  97.749 49.704
#&gt; - wt    1    0.2669  97.997 49.765
#&gt; - hp    1    0.7599  98.490 49.886
#&gt; - cyl   1    2.2946 100.025 50.257
#&gt; - gear  1    8.4767 106.207 51.696
#&gt; &lt;none&gt;               97.730 51.700
#&gt; - drat  1   27.0937 124.824 55.572
#&gt; - carb  1   29.1906 126.921 55.972
#&gt; 
#&gt; Step:  AIC=49.7
#&gt; mpg ~ cyl + disp + hp + drat + wt + gear + carb
#&gt; 
#&gt;        Df Sum of Sq     RSS    AIC
#&gt; - disp  1     0.027  97.759 47.707
#&gt; - wt    1     0.404  98.135 47.799
#&gt; - hp    1     0.777  98.509 47.890
#&gt; - cyl   1     2.675 100.406 48.348
#&gt; &lt;none&gt;               97.731 49.700
#&gt; - gear  1     8.500 106.231 49.702
#&gt; - drat  1    27.092 124.824 53.572
#&gt; - carb  1    33.987 131.719 54.863
#&gt; 
#&gt; Step:  AIC=47.71
#&gt; mpg ~ cyl + hp + drat + wt + gear + carb
#&gt; 
#&gt;        Df Sum of Sq     RSS    AIC
#&gt; - hp    1     1.001  98.759 45.951
#&gt; - wt    1     1.127  98.886 45.982
#&gt; - cyl   1     2.948 100.707 46.420
#&gt; &lt;none&gt;               97.759 47.707
#&gt; - gear  1     8.716 106.475 47.757
#&gt; - drat  1    27.829 125.588 51.719
#&gt; - carb  1    37.084 134.843 53.425
#&gt; 
#&gt; Step:  AIC=45.95
#&gt; mpg ~ cyl + drat + wt + gear + carb
#&gt; 
#&gt;        Df Sum of Sq     RSS    AIC
#&gt; - wt    1     2.052 100.811 44.445
#&gt; - cyl   1     2.122 100.881 44.461
#&gt; &lt;none&gt;               98.759 45.951
#&gt; - gear  1    17.906 116.666 47.950
#&gt; - drat  1    27.261 126.020 49.801
#&gt; - carb  1    47.357 146.116 53.352
#&gt; 
#&gt; Step:  AIC=44.44
#&gt; mpg ~ cyl + drat + gear + carb
#&gt; 
#&gt;        Df Sum of Sq    RSS    AIC
#&gt; - cyl   1     3.846 104.66 43.343
#&gt; &lt;none&gt;              100.81 44.445
#&gt; - gear  1    23.549 124.36 47.483
#&gt; - drat  1    59.197 160.01 53.532
#&gt; - carb  1   115.220 216.03 60.737
#&gt; 
#&gt; Step:  AIC=43.34
#&gt; mpg ~ drat + gear + carb
#&gt; 
#&gt;        Df Sum of Sq    RSS    AIC
#&gt; &lt;none&gt;              104.66 43.343
#&gt; - gear  1    21.151 125.81 45.761
#&gt; - drat  1    57.478 162.14 51.849
#&gt; - carb  1   259.357 364.01 71.259
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">summary</span>(stepwise_mod)
</code></pre></div><pre><code>#&gt; 
#&gt; Call:
#&gt; lm(formula = mpg ~ drat + gear + carb, data = train)
#&gt; 
#&gt; Residuals:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -4.8838 -1.4587  0.0255  1.2761  4.2824 
#&gt; 
#&gt; Coefficients:
#&gt;             Estimate Std. Error t value    Pr(&gt;|t|)    
#&gt; (Intercept)  -3.3923     3.6017  -0.942     0.35750    
#&gt; drat          5.2265     1.5770   3.314     0.00346 ** 
#&gt; gear          3.4169     1.6996   2.010     0.05806 .  
#&gt; carb         -2.7135     0.3854  -7.040 0.000000792 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 2.288 on 20 degrees of freedom
#&gt; Multiple R-squared:  0.8808,	Adjusted R-squared:  0.8629 
#&gt; F-statistic: 49.24 on 3 and 20 DF,  p-value: 0.000000002032
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">vif</span>(stepwise_mod)
</code></pre></div><pre><code>#&gt;     drat     gear     carb 
#&gt; 3.232190 3.220083 1.011370
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">shapiro.test</span>(stepwise_mod<span style="color:#f92672">$</span>residuals)
</code></pre></div><pre><code>#&gt; 
#&gt; 	Shapiro-Wilk normality test
#&gt; 
#&gt; data:  stepwise_mod$residuals
#&gt; W = 0.98755, p-value = 0.9869
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">bptest</span>(stepwise_mod)
</code></pre></div><pre><code>#&gt; 
#&gt; 	studentized Breusch-Pagan test
#&gt; 
#&gt; data:  stepwise_mod
#&gt; BP = 7.1292, df = 3, p-value = 0.06789
</code></pre><h2 id="ridge-regression-model">Ridge Regression Model</h2>
<p>As mentioned above, the second way to overcome the problem of overfitting in regression is to use Regularization Regression.
Two types of regression regularization will be discussed this time, the first is Ridge regression.</p>
<p><strong>Ridge regression is the same as OLS regression</strong></p>
<p>Below, the writer tries to prove whether Ridge has parameters <code>\(\lambda = 0\)</code> then the Ridge regression coefficient is <em>approximately</em> the same as the Ordinary Least Square Regression coefficients.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">ridge_cv <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">glmnet</span>(xtrain, ytrain, alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>,lambda <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>)
<span style="color:#a6e22e">predict.glmnet</span>(ridge_cv, s <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>, type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;</span><span style="color:#e6db74">coefficients&#39;</span>)
</code></pre></div><pre><code>#&gt; 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
#&gt;                        1
#&gt; (Intercept) -5.105168916
#&gt; cyl          0.675181160
#&gt; disp        -0.001095199
#&gt; hp          -0.009943070
#&gt; drat         5.323214337
#&gt; wt          -0.475257511
#&gt; qsec         0.004891085
#&gt; gear         3.456626642
#&gt; carb        -2.660723992
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">ols<span style="color:#f92672">$</span>coefficients
</code></pre></div><pre><code>#&gt;  (Intercept)          cyl         disp           hp         drat 
#&gt; -5.422701011  0.696306643 -0.001029893 -0.010342436  5.368257224 
#&gt;           wt         qsec         gear         carb 
#&gt; -0.475646420  0.011715279  3.440558813 -2.661675645
</code></pre><h3 id="choosing-optimal-lambda-value-of-ridge-regression">Choosing Optimal Lambda Value of Ridge Regression</h3>
<p>As already discussed <code>\(\lambda\)</code> is an error value parameter used to add bias so that variance and estimator decrease.</p>
<p>The glmnet function trains the model multiple times for all the different values of lambda which we pass as a sequence of vector to the lambda = argument in the glmnet function.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">set.seed</span>(<span style="color:#ae81ff">100</span>)
lambdas_to_try <span style="color:#f92672">&lt;-</span> <span style="color:#ae81ff">10</span><span style="color:#a6e22e">^seq</span>(<span style="color:#ae81ff">-3</span>, <span style="color:#ae81ff">7</span>, length.out <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>)
</code></pre></div><p>The next task is to identify the optimal value of lambda which results into minimum error. This can be achieved automatically by using <code>cv.glmnet()</code> function. Setting <strong>alpha = 0 implements Ridge Regression</strong>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">set.seed</span>(<span style="color:#ae81ff">100</span>)
ridge_cv <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cv.glmnet</span>(xtrain, ytrain, alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>, lambda <span style="color:#f92672">=</span> lambdas_to_try)
<span style="color:#a6e22e">plot</span>(ridge_cv)
</code></pre></div><p><!-- raw HTML omitted --></p>
<p>From the picture above it shows the best log (lambda) between about 0 to 2.5 because it has the smallest MSE value. On the x-axis (above) you can see all the numbers are 8. This shows the number of predictors used. No matter how much lambda is used, the predictor variable remains 8. This means that Ridge Regression cannot perform automatic feature selection.</p>
<h3 id="extracting-the-best-lambda-of-ridge-regression">Extracting the Best Lambda of Ridge Regression</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">best_lambda_ridge <span style="color:#f92672">&lt;-</span> ridge_cv<span style="color:#f92672">$</span>lambda.min
best_lambda_ridge
</code></pre></div><pre><code>#&gt; [1] 1.707353
</code></pre><h3 id="build-models-based-on-the-best-lambda">Build Models Based on The Best Lambda</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">ridge_mod <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">glmnet</span>(xtrain, ytrain, alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>, lambda <span style="color:#f92672">=</span> best_lambda_ridge)
<span style="color:#a6e22e">predict.glmnet</span>(ridge_mod,  type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;</span><span style="color:#e6db74">coefficients&#39;</span>)
</code></pre></div><pre><code>#&gt; 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
#&gt;                       s0
#&gt; (Intercept) 17.201163649
#&gt; cyl         -0.354685636
#&gt; disp        -0.004855094
#&gt; hp          -0.013125376
#&gt; drat         2.377336631
#&gt; wt          -1.163291643
#&gt; qsec         0.060788898
#&gt; gear         1.415532975
#&gt; carb        -1.065339475
</code></pre><p>So the Ridge Regression model obtained is <code>$$mpg = 17.81 - 0.39cyl -0.005disp-0.01hp+2.05drat-1.12wt+0.09qseq+1.32gear-0.90carb$$</code></p>
<h2 id="lasso-regression-model">LASSO Regression Model</h2>
<h3 id="choosing-optimal-lambda-value-of-lasso-regression">Choosing Optimal Lambda Value of LASSO Regression</h3>
<p>Setting the range of lambda values, with the same parameter settings as Ridge Regression</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">lambdas_to_try <span style="color:#f92672">&lt;-</span> <span style="color:#ae81ff">10</span><span style="color:#a6e22e">^seq</span>(<span style="color:#ae81ff">-3</span>, <span style="color:#ae81ff">7</span>, length.out <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>)
</code></pre></div><p>Identify the optimal value of lambda which results into minimum error. This can be achieved automatically by using <code>cv.glmnet()</code> function. Setting <strong>alpha = 1 implements LASSO Regression</strong>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">set.seed</span>(<span style="color:#ae81ff">100</span>)
lasso_cv <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cv.glmnet</span>(xtrain, ytrain, alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>, lambda <span style="color:#f92672">=</span> lambdas_to_try)
<span style="color:#a6e22e">plot</span>(lasso_cv)
</code></pre></div><p><!-- raw HTML omitted --></p>
<p>From the picture above it shows the best log(lambda) between about -1 to 0, because it has the smallest MSE value. On the x-axis (above) has a different value, based on the best lambda it can be seen that the best predictor variable is 5. This shows that LASSO Regression can perform automatic feature selection.</p>
<h3 id="extracting-the-est-lambda-of-lasso-regression">Extracting the est lambda of LASSO Regression</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">best_lambda_lasso <span style="color:#f92672">&lt;-</span> lasso_cv<span style="color:#f92672">$</span>lambda.min
best_lambda_lasso
</code></pre></div><pre><code>#&gt; [1] 0.1668101
</code></pre><h3 id="build-models-based-the-best-lambda">Build Models Based The Best Lambda</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># Fit final model, get its sum of squared residuals and multiple R-squared</span>
lasso_mod <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">glmnet</span>(xtrain, ytrain, alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>, lambda <span style="color:#f92672">=</span> best_lambda_lasso)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">predict.glmnet</span>(lasso_mod, type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;</span><span style="color:#e6db74">coefficients&#39;</span>)
</code></pre></div><pre><code>#&gt; 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
#&gt;                      s0
#&gt; (Intercept)  7.37367761
#&gt; cyl          .         
#&gt; disp         .         
#&gt; hp          -0.01044739
#&gt; drat         4.12926425
#&gt; wt          -0.97081377
#&gt; qsec         .         
#&gt; gear         2.14674952
#&gt; carb        -1.88455227
</code></pre><p>From the results above it can be seen that the variables cyl, disp, and qseq have decreased coefficients to exactly 0. So the LASSO Regression model obtained is
<code>$$mpg = 8.31-0.001hp+4.063drat-1.00wt+1.96gear-1.79carb$$</code></p>
<h2 id="model-comparison">Model Comparison</h2>
<h3 id="compare-the-mse-value-of-data-train-and-test">Compare The MSE value of Data Train and Test</h3>
<p><strong>OLS Regression</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># MSE of Data Train (OLS)</span>
ols_pred <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">predict</span>(ols, newdata <span style="color:#f92672">=</span> train)
<span style="color:#a6e22e">mean</span>((ols_pred<span style="color:#f92672">-</span>ytrain)^2)
</code></pre></div><pre><code>#&gt; [1] 4.072081
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># MSE of Data Test (OLS)</span>
ols_pred <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">predict</span>(ols, newdata <span style="color:#f92672">=</span> test)
<span style="color:#a6e22e">mean</span>((ols_pred<span style="color:#f92672">-</span>ytest)^2)
</code></pre></div><pre><code>#&gt; [1] 22.15188
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># MSE of Data Train (Stepwise)</span>
back_pred <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">predict</span>(stepwise_mod, newdata <span style="color:#f92672">=</span> train)
<span style="color:#a6e22e">mean</span>((back_pred<span style="color:#f92672">-</span>ytrain)^2)
</code></pre></div><pre><code>#&gt; [1] 4.360721
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># MSE of Data Test (Stepwise)</span>
back_pred <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">predict</span>(stepwise_mod, newdata <span style="color:#f92672">=</span> test)
<span style="color:#a6e22e">mean</span>((back_pred<span style="color:#f92672">-</span>ytest)^2)
</code></pre></div><pre><code>#&gt; [1] 22.41245
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># MSE of Data Train (Ridge)</span>
ridge_pred <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">predict</span>(ridge_mod,  newx <span style="color:#f92672">=</span> xtrain)
<span style="color:#a6e22e">mean</span>((ridge_pred<span style="color:#f92672">-</span>ytrain)^2)
</code></pre></div><pre><code>#&gt; [1] 4.920852
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># MSE of Data Test (Ridge)</span>
ridge_pred <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">predict</span>(ridge_mod,  newx <span style="color:#f92672">=</span> xtest)
<span style="color:#a6e22e">mean</span>((ridge_pred<span style="color:#f92672">-</span>ytest)^2)
</code></pre></div><pre><code>#&gt; [1] 7.22441
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># MSE of Data Train (LASSO)</span>
lasso_pred <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">predict</span>(lasso_mod,  newx <span style="color:#f92672">=</span> xtrain)
<span style="color:#a6e22e">mean</span>((lasso_pred<span style="color:#f92672">-</span>ytrain)^2)
</code></pre></div><pre><code>#&gt; [1] 4.269417
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># MSE of Data Test (LASSO)</span>
lasso_pred <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">predict</span>(lasso_mod,  newx <span style="color:#f92672">=</span> xtest)
<span style="color:#a6e22e">mean</span>((lasso_pred<span style="color:#f92672">-</span>ytest)^2)
</code></pre></div><pre><code>#&gt; [1] 13.56875
</code></pre><p>Based on the above results the best regression model using <code>mtcars</code> data is Ridge Regression. Because the difference of MSE data training and testing is not much different.</p>
<h3 id="compare-prediction-results-from-all-four-models">Compare prediction results from all four models.</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">predict_value <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cbind</span>(ytest, ols_pred, ridge_pred, lasso_pred,back_pred)
<span style="color:#a6e22e">colnames</span>(predict_value) <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">c</span>(<span style="color:#e6db74">&#34;</span><span style="color:#e6db74">y_actual&#34;</span>, <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">ols_pred&#34;</span>, <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">ridge_pred&#34;</span>, <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">lasso_pred&#34;</span>, <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">stepwise_pred&#34;</span>)
predict_value
</code></pre></div><pre><code>#&gt;                  y_actual  ols_pred ridge_pred lasso_pred stepwise_pred
#&gt; Pontiac Firebird     19.2 17.829311   16.12641   17.20188      17.52899
#&gt; Fiat X1-9            27.3 28.902655   27.72686   28.35547      28.88586
#&gt; Porsche 914-2        26.0 31.136052   28.00826   29.60271      31.41857
#&gt; Lotus Europa         30.4 27.691995   27.01435   27.25625      27.96911
#&gt; Ford Pantera L       15.8 24.928066   19.23691   22.15912      24.89406
#&gt; Ferrari Dino         19.7 16.325756   19.08362   17.23060      16.33122
#&gt; Maserati Bora        15.0  9.759043   12.21058   10.68292      10.48615
#&gt; Volvo 142E           21.4 25.508611   24.96332   25.32522      26.32918
</code></pre><p>Based on the results that can be seen in the model that produces the closest prediction value <code>y_actual</code> is the <strong>Ridge Regression</strong> model.</p>
<p>Based on the comparison of the Error values from the models, it was found that the Ridge Regression model which has the smallest Mean Square Error (MSE).</p>
<h2 id="conclusion">Conclusion</h2>
<p>Based on the description above, the following conclusions are obtained:</p>
<ol>
<li>
<p>Multicollinearity problems in the Ordinary Least Square (OLS) regression model will make the predictor estimator have a large variance, causing overfitting problems.</p>
</li>
<li>
<p>Ridge and LASSO regression are good enough to be applied as an alternative if our Ordinary Least Square (OLS) model has multicollinearity problems.</p>
</li>
<li>
<p>Ridge and LASSO regression work by adding the bias parameter (λ) so that the estimator variance is reduced.</p>
</li>
<li>
<p>Ridge and LASSO Regression is that if ridge regression can shrink the coefficient close to 0 so that all predictor variables are retained.</p>
</li>
<li>
<p>LASSO Regression can shrink the coefficient to exactly 0, so that LASSO can select and discard the predictor variables.</p>
</li>
<li>
<p>Ridge Regression is best used if the data do not have many predictor variables, whereas LASSO Regression is good if the data has many predictor variables, because it will simplify the interpretation of the model.</p>
</li>
<li>
<p>From the comparison of the above models between OLS Regression, Stepwise Regression, Ridge Regression, and LASSO Regression, the best model is Ridge Regression. Based on the MSE value of the smallest model and the closest prediction estimator to the actual value.</p>
</li>
</ol>
<h2 id="annotations">Annotations</h2>
<ol>
<li>
<p>Shenoy, Aditi, 2019, What is Bias, Variance and Bias-Variance Tradeoff? (Part 1), viewed 2019, <a href="https://aditishenoy.github.io/what-is-bias-variance-and-bias-variance-tradeoff-part-1/">https://aditishenoy.github.io/what-is-bias-variance-and-bias-variance-tradeoff-part-1/</a></p>
</li>
<li>
<p>bbroto06, Predicting Labour Wages using Ridge and Lasso Regression, viewed 2019, <a href="http://rpubs.com/bbroto06/ridge_lasso_regression">http://rpubs.com/bbroto06/ridge_lasso_regression</a></p>
</li>
<li>
<p>Vidhya, Analytics , 2019, A comprehensive beginners guide for Linear, Ridge and Lasso Regression in Python and R, viewed 2019, <a href="https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/">https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/</a></p>
</li>
</ol>

  </div>

  <footer>
    <ul class="stats">
  <li class="categories">
    <ul>
        
            
            
                <i class="fa fa-folder"></i>
                
                
                <li><a class="article-category-link" href="/categories/r">R</a></li>
                
            
        
    </ul>
  </li>
  <li class="tags">
    <ul>
        
            
            
                <i class="fa fa-tags"></i>
                
                
                <li><a class="article-category-link" href="/tags/regression">regression</a></li>
                
                
                <li><a class="article-category-link" href="/tags/machine-learning">Machine Learning</a></li>
                
                
                <li><a class="article-category-link" href="/tags/overfitting">overfitting</a></li>
                
                
                <li><a class="article-category-link" href="/tags/regularization">Regularization</a></li>
                
            
        
    </ul>
  </li>
</ul>

  </footer>

</article>

    <article class="post">
        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "https-algotech-netlify-com" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </article>


<ul class="actions pagination">
    
        <li><a href="/blog/bio-intro/"
                class="button big previous">Bioinformatics: Decoding Nature&#39;s Code of Life</a></li>
    

    
        <li><a href="/blog/introduction-to-generative-adversarial-network-with-keras/"
                class="button big next">Introduction to Generative Adversarial Network with Keras</a></li>
    
</ul>


    </div>
    
<section id="sidebar">

  
  <section id="intro">
    
    
      
        <a href='/'><img src="/img/main/logo.png" class="intro-circle" width="30%" alt="Hugo Future Imperfect" /></a>
      
    
    
      <header>
        <h2>Algoritma Technical Blog</h2>
        <p>We're a group of people who teach data science to individuals, trains companies and their employees to better profit from data. We care about the development of data science and a sense of community that connects our alumni and team with one another. To learn more about our approach to data science problems, feel free to hop over to our blog.</p>
      </header>
    
    
      <ul class="icons">
        
        
  <li><a href="//github.com/teamalgoritma" target="_blank" title="GitHub" class="fa fa-github"></a></li>



























  <li><a href="//linkedin.com/company/teamalgoritma" target="_blank" title="LinkedIn Company" class="fa fa-linkedin"></a></li>









  <li><a href="//facebook.com/teamalgoritma" target="_blank" title="Facebook" class="fa fa-facebook"></a></li>





















  <li><a href="//instagram.com/teamalgoritma" target="_blank" title="Instagram" class="fa fa-instagram"></a></li>





  <li><a href="//twitter.com/teamalgoritma" target="_blank" title="Twitter" class="fa fa-twitter"></a></li>




















      </ul>
    
  </section>



  
  
  

  
  

  
  <section id="footer">
    <p class="copyright">
      
        &copy; 2020
        
          Algoritma Technical Blog
        
      .
      Powered by <a href="//gohugo.io" target="_blank">Hugo</a>
    </p>
  </section>
</section>

    </div>
    <a id="back-to-top" href="#" class="fa fa-arrow-up fa-border fa-2x"></a>
    

    
      
    

    
      
      
      
        <script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>
        
        
        
        <script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
        <script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
        <script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/css.min.js"></script>
        <script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>
      
    
    
    
      <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/skel/3.0.1/skel.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.1.25/jquery.fancybox.min.js"></script>
      <script src="/js/util.js"></script>
      <script src="/js/main.js"></script>
      <script src="/js/backToTop.js"></script>
    

    
      
        
      
        
          <script src="/js/bootstrap.min.js"></script>
        
      
    

    
    <script>hljs.initHighlightingOnLoad();</script>
      <script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


  </body>
</html>

