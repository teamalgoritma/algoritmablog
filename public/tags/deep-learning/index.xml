<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep learning on Algoritma Technical Blog</title>
    <link>/tags/deep-learning/</link>
    <description>Recent content in deep learning on Algoritma Technical Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Dec 2019 00:00:00 +0000</lastBuildDate><atom:link href="/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Introduction to Generative Adversarial Network with Keras</title>
      <link>/blog/introduction-to-generative-adversarial-network-with-keras/</link>
      <pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/introduction-to-generative-adversarial-network-with-keras/</guid>
      <description>In 2018 a paint of Edmond de Belamy made by machine learning (GAN) was sold for $432,500 in online auction, Christie&amp;rsquo;s. This made Chritie&amp;rsquo;s the first auction house that sell works created by machine learning. On an unbelievable price. What do you think about this ? Will machine learning help us create arts, or will it kill our creativity?
Intro  Is artificial intelligence set to become artâ€™s next medium? - Chritie&amp;rsquo;s</description>
    </item>
    
    <item>
      <title>Text Classification with LSTM</title>
      <link>/blog/text-lstm/</link>
      <pubDate>Tue, 12 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/text-lstm/</guid>
      <description>Deep Neural Network Before we further discuss the Long Short-Term Memory Model, we will first discuss the term of Deep learning where the main idea is on the Neural Network. So Neural Network is one branch of machine learning where the learning process imitates the way neurons in the human brain works. In Neural Network we know several terms, such as the input layer, hidden layer, and output layer. So the different betweetn Deep Learning and Neural Network architecture is the number of hidden layers specified.</description>
    </item>
    
    <item>
      <title>Time Series Prediction with LSTM</title>
      <link>/blog/time-series-prediction-with-lstm/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/time-series-prediction-with-lstm/</guid>
      <description>Time Series Forecasting using LSTMTime series involves data collected sequentially in time. In Feed Forward Neural Network we describe that all inputs are not dependent on each other or are usually familiar as IID (Independent Identical Distributed), so it is not appropriate to use sequential data processing.A Recurrent Neural Network (RNN) deals with sequence problems because their connections form a directed cycle.</description>
    </item>
    
  </channel>
</rss>
