<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LSTM on Algoritma Technical Blog</title>
    <link>/tags/lstm/</link>
    <description>Recent content in LSTM on Algoritma Technical Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 12 Nov 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/lstm/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Text Classification with LSTM</title>
      <link>/blog/text-lstm/</link>
      <pubDate>Tue, 12 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/text-lstm/</guid>
      <description>Deep Neural Network Before we further discuss the Long Short-Term Memory Model, we will first discuss the term of Deep learning where the main idea is on the Neural Network. So Neural Network is one branch of machine learning where the learning process imitates the way neurons in the human brain works. In Neural Network we know several terms, such as the input layer, hidden layer, and output layer. So the different betweetn Deep Learning and Neural Network architecture is the number of hidden layers specified.</description>
    </item>
    
    <item>
      <title>Time Series Prediction with LSTM</title>
      <link>/blog/time-series-prediction-with-lstm/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/time-series-prediction-with-lstm/</guid>
      <description>Time Series Forecasting using LSTMTime series involves data collected sequentially in time. In Feed Forward Neural Network we describe that all inputs are not dependent on each other or are usually familiar as IID (Independent Identical Distributed), so it is not appropriate to use sequential data processing.A Recurrent Neural Network (RNN) deals with sequence problems because their connections form a directed cycle.</description>
    </item>
    
  </channel>
</rss>