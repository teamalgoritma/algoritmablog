<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>lime on Algoritma Technical Blog</title>
    <link>/tags/lime/</link>
    <description>Recent content in lime on Algoritma Technical Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Dec 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/lime/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Interpreting Classification Model with LIME</title>
      <link>/blog/interpreting-classification-model-with-lime/</link>
      <pubDate>Mon, 02 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/interpreting-classification-model-with-lime/</guid>
      <description>Introduction One of many things to consider when we want to choose a machine learning model is the interpretability: can we analyze what variables or certain values that contribute toward particular class or target? Some models can be easily interpreted, such as the linear or logistic regression model and decision trees, but interpreting more complex model such as random forest and neural network can be challenging. This sometimes drive the data scientist to choose more interpretable model since they need to communicate it to their manager or higher rank, who perhaps are not familiar with machine learning.</description>
    </item>
    
  </channel>
</rss>