<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>recall on Algoritma Technical Blog</title>
    <link>https://algotech.netlify.com/tags/recall/</link>
    <description>Recent content in recall on Algoritma Technical Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 10 Jan 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://algotech.netlify.com/tags/recall/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Metrics Evaluation using `yardstick`</title>
      <link>https://algotech.netlify.com/blog/metrics-evaluation-using-yardstick/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://algotech.netlify.com/blog/metrics-evaluation-using-yardstick/</guid>
      <description>Motivation Evaluating your machine learning algorithms is important part in your project. Choice of metrics influences how the performance of machine learning algorithms is measured and compared. Metrics evaluation used to measure the performance of our algorithms. For Regression models, we usually use R-squared and MSE, but for Classification models we can use precision, recall and accuracy. Evaluating a classifier is often much more difficult than evaluating a regression algorithm.</description>
    </item>
    
  </channel>
</rss>
