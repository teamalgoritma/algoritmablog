---
title: Boosting Algorithm (AdaBoost and XGBoost)
author: Ajeng Prastiwi, David Tahi Ulubalang
github: https://github.com/ajengprstw/xgboost
date: '2020-11-23'
slug: xgboost
categories:
  - R
tags: 
  - Machine Learning
description: ''
featured: 'boosting-algorithm.png'
featuredalt: ''
featuredpath: 'date'
linktitle: ''
type: post
---

<style>
body {
text-align: justify}
</style>


```{r setup, include=FALSE}
# clear-up the environment
rm(list = ls())
# chunk options
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  comment = "#>"
)
options(scipen = 99)
```

# Introduction {.tabset}

## What is boosting?

Boosting is an ensemble method of converting weak learners into strong learners. Weak and strong refer to a measure how correlated are the learners to the actual target variable[^1]. In boosting, each training sample are used to train one unit of decision tree and picked with replacement over-weighted data. The trees will learn from predecessors and updates the residuals error.


## Learning Objectives

The goal of this article is to help you:

- Understand the concept of boosting    
- Compare boosting and bagging method    
- Understand how AdaBoost algorithm works    
- Understand how XGBoost algorithm works    
- Implement AdaBoost and Xgboost in business case    

<center>![](/img/xgboost/concept_map.png)</center>

## Library and setup

```{r message=F, warning=F}
library(tidyverse)
library(rsample)
library(xgboost)
library(ggthemes)
library(tictoc)
library(fastAdaboost)
library(tidymodels)
library(inspectdf)
library(caret)
theme_set(theme_pander())
```

# Bagging vs Boosting

The idea of bagging is creating many subsets of training sample with replacement, each observation has the same probability to picked as sample. Then, each training sample are used to train one unit of decision tree and use the average of all the predictions. In boosting, each training sample are used to train one unit of decision tree and picked with replacement over-weighted data. The trees will learn from predecessors and updates the residuals error. After these weak learners are trained, a weighted average of their estimates are taken for the final predictions at the end[^2].

<center> ![](/img/xgboost/bagging.png) </center>

<center> ![](/img/xgboost/boostingg.png) </center>

# Boosting Method

The different method of boosting algorithm are "How they create the weak learners during the iterative process":

## AdaBoost

Adaptive boosting was formulated by Yoav Freund and Robet Schapire. AdaBoost was the first practical boosting algorithm, and remains one of the most widely used and studied, with applications in numerous fields. AdaBoost algorithm works on changes the sample distribution by modifying weight data points for each iteration.

### How AdaBoost Works?

We can split the idea of AdaBoost into 3 big concept : 

#### 1. Used Stump as Weak Learners 

Weak learners is any model that has a accuracy better than random guessing even if it is just slightly better (e.g 0.51). In an Ensemble methods we combines multiple weak learners to make strong learner model. In AdaBoost, weak learners are used, a 1-level decision tree (Stump).The main idea when creating a weak classifier is to find the best stump that can separate data by minimizing overall errors.

<center> ![](/img/xgboost/ada1.png) </center>

#### 2. Influence the Next Stump

Unlike bagging, which makes models in parallel, Boosting does training sequentially, which means that each stump (weak learner) is affected by the previous stump. The way Stump affects the next stump is by giving different weights to the data that will be used in the next stump maknig process. This weighting is based on error calculations, if a data is incorrectly predicted in the first stump, then the data will be given a greater weight in the next stump-making process.

![](/img/xgboost/ada2.png)

#### 3. Weighted Vote


In AdaBoost algorithm, each stump has a different weight, the weight for each stump is based on the resulting error rate. The smaller errors generated by a stump, the greater the weight of the stump. The weight of each stump is used in the voting process, if the greater the total weight obtained by one of the classes, then that class will be used as the final class.

![](/img/xgboost/ada3.png)

### Case Example using AdaBoost

The hotel is one of the lodgings most often used when traveling. With limited hotel capacity, canceling a reservation can be detrimental to the person providing the hotel service. In this case, we will predict hotel cancellations using data [Hotel Reservation Requests](https://www.kaggle.com/jessemostipak/hotel-booking-demand) taken from Kaggle.

```{r}
booking <- read.csv("data_input/xgboost/hotel_bookings.csv", stringsAsFactors = T) 
head(booking)
```
The data contains 119390 observations and  32 variables. Here some description of each feature:

- `hotel`: Hotel (H1 = Resort Hotel or H2 = City Hotel)  

- `is_canceled`: Value indicating if the booking was canceled (1) or not (0)    

- `lead_time`: Number of days that elapses between the entering date of the booking into the PMS and the arrival date  

- `arrival_date_year`: Year of arrival date    

- `arrival_date_month`: Month of arrival data    

- `arrival_date_week_number`: Week number of year for arrival date    

- `arrival_date_day_of_month`: Day of arrival date    

- `stays_in_weekend_nights`: Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel

- `adults`: Number of adults

- `children`: Number of children

- `babies`: Number of babies

- `meal`: Type of meal booked. Categories are presented in standard hospitality meal packages:
    - Undefined/SC : no meal package;
    - BB : Bed & Breakfast;
    - HB : Half board (breakfast and one other meal-usually dinner);
    - FB : Full board (breakfast, lunch, and dinner)

- `country`: Country of origin. Categories are represented in the ISO 3155-3:2013 format

- `market_segment`: Market segment designation. In categories, the term "TA" means "Travel agents" and "TO" means "Tour Operators"

- `distribution_channel`: Booking distribution channel. The term "TA" means "Travel Agents" and "TO" means "Tour Operators"

- `is_repeated_guest`: Value indicating if the booking name was from a repeated guest (1) or not (0)

- `previous_cancellations`: Number of previous bookings that were cancelled bu the customer prior to the current booking

- `previous_bookings_not_canceled`: Number of previous bookings not cancelled by the customer prior to the current booking

- `reserved_room_type`: Code of room type reserved. Code is represented instead of designation for anonymity reasons

- `assigned_room_type`: Code for the type of room assigned to the booking. Sometimes the assigned room type differs from the reserved room type due to hotel opeartions reasons (e.g overbooking) or by customer request. Code is presented instead of designation for anonymity reasons

- `booking_changes`: Number of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in cancellation

- `deposit_type`: Indication on if the customer made a deposit to guarantee the booking. This variable can assume three categories:
    - No deposit - no deposit was made;
    - Non refund - a deposit was made in the value of the total stay cost;
    - Refundable - a deposit was made with a value under the total cost of stay

- `agent`: ID of the travel agency that made the booking

- `company`: ID of the company/entity that made the booking or responsible for paying the booking. ID is presented instad of designation for anonymity reasons

- `days_in_waiting_list`: Number of days the booking was in the waiting list before it was confirmed to the customer

- `customer_type`: Type of booking, assuming one of four categories:
    - Contract - when the booking has an allotment or other type of contract associated to it;
    - Group - when the booking is associated to a group;
    - Transient - when the booking is not part of a group or contract, and is not associated to other transient booking
    - Transient-party - when the booking is transient, but is associated to at least other transient booking

- `adr`: Average daily rate as defined by dividing the sum of all lodging transactions by the total number of staying nights

- `required_car_parking_spaces`: Number of car parking spaces required by the customer

- `total_of_special_requests`: Number of special requests made by the customer (e.g. twin bed or high floor)

- `reservation_status`: Reservation las status, assuming one of three categories:
  - Canceled - booking was canceled by the customer;
  - Check-out - customer has checked in but already departed;
  - No-Show - customer did not check-in and did inform the hotel of the reason why

- `reservation_status_date`: Date a which the last status was set. This variable can be used in conjuction with the reservation status to understand when was the booking canceled or when did the customer checked-out of the model.
    
The model prediction will help hotel to predict the guest will **cancel or not cancel** the booking hotel. We will remove variables `agent` and `company` because there are have a lot of levels, and also we remove `reservation_status` and `reservation_status_date`.

```{r}
booking <- booking %>% 
          select(-c(reservation_status_date, agent, company,
                    reservation_status)) %>% 
          mutate(is_canceled = as.factor(is_canceled))
```


#### Exploratory Data Analysis

Before we go further, we need to check if there are any missing values in data. We can used `inspect_na()` function from `inspectdf` package to check the missing value. 

```{r}
booking %>% 
  inspect_na()
```


From the result above variable `children` have missing values with 4 observation, let's fill the missing value with the 0.
```{r}
booking <- booking %>% 
           mutate(children = replace_na(children,0))
```

Now let's check the condition of the categorical variable using `inspect_cat()` function.
```{r}
booking %>% 
  inspect_cat()
```

From the result above, the country column has 178 unique value. We will reduce the unique value of the `country` to 11, namely by taking the 10 countries that appear most frequently and other countries will be changed to **other**.

```{r}
booking <- booking %>% 
  mutate(country = fct_lump_n(country, n = 10)) 

booking %>% 
  inspect_cat() %>% 
  show_plot()
```

Before we do the modeling, let's first check the proportions of the target class to find out how balanced the target class.

```{r}
booking %>% 
  pull(is_canceled) %>% 
  table() %>% 
  prop.table()
```

Class with label 0 has a proportion of about 63% of the data while class with label 1 has a proportion of 37%, this shows that class with label 0 is more dominant.

#### Modelling

We’ll create our training and testing data using `initial_split` function

```{r}
set.seed(100)
splitted <- initial_split(booking, prop = 0.8,strata = is_canceled)
data_train <- training(splitted)
data_test <- testing(splitted)

```

The function used to create the AdaBoost model is `adaboost()` from the `fastAdaboost` package. There are 3 parameters that can be filled in this function:     
- `formula`: Formula for models  
- `data`: Data used in the modeling process     
- `nIter`: Number of stumps used on the model     

```{r eval=F}

model_ada <- adaboost(formula = is_canceled~.,
                      data = data_train, 
                      nIter = 100)

```


```{r echo=F, eval = F}
# read kalau membutuhkan aja
model_ada <- readRDS("data_input/xgboost/adamodel2.rds")
```

As we know each stump in the model has a different weight, the weight of each stump can be seen in `model_ada$weights`. When the weights are visualized, it will be seen that the stump formed at the end of the iteration has a smaller weight when compared to the stump formed at the beginning of the iteration.

```{r eval=F}
plot_weights <- data.frame(stump_id = c(1:100), 
           weight = model_ada$weights) %>% 
  ggplot(aes(y = weight, x = stump_id)) +
  geom_col(fill = "dodgerblue3")
plot_weights
```

```{r echo=F}
readRDS("data_input/xgboost/plot_weight.RDS")
```

Now let's predict the test dataset
```{r eval=F}
pred_hotel <- predict(object = model_ada, newdata = data_test)
str(pred_hotel)
```

```{r echo=FALSE}
pred_hotel <- readRDS("data_input/xgboost/pred_hotel.rds")
str(pred_hotel)
```

the predicted object has several components :     
- $votes : Total weighted votes achieved by each class    
- $class : The class predicted by the classifier     
- $prob  : A matrix with predicted probability of each class for each observation    
- $eror  : The error on the test data if labeled. (1-accuracy)    

Now let's check how good our model using confusion matrix

```{r}
confusionMatrix(data = pred_hotel$class, reference = data_test$is_canceled, positive = "1")
```
Based on the confusion matrix above, we know that the accuracy of model is 0.88. Since we know that our data is dominated by the class labeled 0 (67%) we have to use another metrics to find out how well our model predicts the two classes. we're going to use the AUC.

```{r}
pred_df <- pred_hotel$prob %>% 
  as.data.frame() %>% 
  rename(class0 = V1, 
         class1 = V2) %>% 
  mutate(predicted = pred_hotel$class, 
         actual = data_test$is_canceled)

auc_ada <- roc_auc(data = pred_df, truth = actual,class1) 
auc_ada
```
The AUC results show that the model formed is good at predicting the target class, this is indicated by the AUC value of 0.95 (the closer to 1 the better). 

AdaBoost has a lot of advantages, mainly it is easier to use with less need for tweaking parameters unlike algorithms like XGBoost. AdaBoost also can reduce the variance in testing data.  


## XGBoost

XGBoost was formulated by Tianqi Chen which started as a research project a part of *The Distributed Deep Machine Leaning Community (DMLC) grop*. XGBoost is one of popular algorithm because it has been the winning algorithm in a number of recent Kaggle competitions. XGBoost is a specific implementation of the Gradient Boosting Model which uses more accurate approximations to find the best tree model[^2]. XGBoost specifically used a more regularized model formalization to control overfitting, which gives it better perfomance.

# How XGBoost works?

<center> ![](/img/xgboost/xgboost.png) </center>

System Optimization: [^5]

1. Parallelized tree building

XGBoost approaches the process of sequential tree building using parrellelized implementation.

2. Tree pruning

Unlike GBM, where tree pruning stops once a negative loss is encountered, XGBoost grows the tree up to max_depth and then prune backward until the improvement in loss function is below a threshold.

3. Cache awareness and out of core computing

XGBoost has been designed to efficiently reduce computing time and allocate an optimal usage of memory resources. This is accomplished by cache awareness by allocating internal buffers in each thread to store gradient statistics. Further enhancements such as ‘out-of-core’ computing optimize available disk space while handling big data-frames that do not fit into memory.

4. Regularization

The biggest advantage of xgboost is regularization. Regularization is a technique used to avoid overfitting in linear and tree based models which limits, regulates or shrink the estimated coefficient towards zero.

5. Handles missing value 

This algorithm has important features of handling missing values by learns the best direction for missing values. The missing values are treated them to combine a sparsity-aware split finding algorithm to handle different types of sparsity patterns in data.

6. Built-in cross validation

The algorithm comes with built in cross validation method at each iteration, taking away the need to explicitly program this search and to specify the exact number of boosting iterations required in a single run.


## Regularization and training loss

XGBoost offers additional regularization term controls the complexity of the model, which help us to avoid overfitting. The objective function is to measure how well the model fit the training data. They consist of two parts: training loss and the regularization term:

$obj(\theta )= L(\theta )+\Omega (\theta )$

Where $L$ is `training loss function` and $\Omega$ is regularization. `Training loss function` measures how well model fit on training data,  $\Omega$ will reduce the complexity of the tree functions.[^3]

For regression case, `training loss function` will obtain from `Mean Squared Error` value:

$L(\theta ) = {\sum_i^{n}(y_i-\hat{y}_i)^2}$

Another loss function for classification case:

$L(\theta ) = {\sum_i[y_iln(1+e^{-\hat{y}_i})+(1-y_i)ln(1+e^{\hat{y}_i})]}$

## Case Example using XGBoost

### Modelling

```{r}
set.seed(100)
splitted <- initial_split(booking, prop = 0.8,strata = is_canceled)
data_train <- training(splitted)
data_test <- testing(splitted)

```

Split the target variable into `label_train` and `label_test`

```{r}
label_train <- as.numeric(as.character(data_train$is_canceled))
label_test <- as.numeric(as.character(data_test$is_canceled))
```

The most important thing when we work with XGBoost is converting the data to Dmatrix, because XGBoost requires a matrix input for the features.
```{r}
# convert data to matrix
train_matrix <- data.matrix(data_train[,-2])
test_matrix <- data.matrix(data_test[,-2])
# convert data to Dmatrix
dtrain <- xgb.DMatrix(data = train_matrix, label = label_train)
dtest <- xgb.DMatrix(data = test_matrix, label = label_test)
```


#### Tuning Parameters

There is no benchmark to define the ideal parameters because it will depend on your data and specific problem. XGBoost Parameters can defined into three categories:[^6]

##### General Parameters

Controls the booster type in the model which eventually drives overall functioning

1. booster

For classification problems, we can use `gbtree` parameter. In `gbtree` a tree is grown one after other and attempts to reduce misclassification reate in subsequent iterations. The next tree is built by giving a higher weight to misclassified points by the previous tree.

For regression problems, we can use `gbtree` and `gblinear`. In `gblinear`, it builds a generalized linear model and optimizes it using regularization and gradient descent. The next model will built on residuals generated by previous iterations.

2. nthread

To enable parallel computing. The default is the maximum number of cores available

3. verbosity

Verbosity to display warning messages.The default value is 1 (warning), 0 for silent, 2 for info, and 3 for debug.


##### Booster Parameters:

Controls the performance of the selected booster

1. Eta

The range of eta is 0 to 1 and default value is 0.3. It controls the maximum number of iterations, the lower eta will generate the slower computation.

2. Gamma

The range of gamma is 0 to infinite and default value is 0 (no regularization). The higher gamma is the higher regularization, regularization means penalizing large coefficients that don't improve the model's performance.

3. nrounds 

it refers to controls the maximum number of iterations.

4. nfold

The number of observation data is randomly partitioned into `nfold` equal size subsamples

5. max_depth

Maximum depth of a tree. The range of max_depth is 0 to infinite and default value is 6, increasing this value will make the model more complex and more likely to overfit.

6. Min_child_weight 

The range of min_child_weight is 0 to infinite and default value is 1. If the leaf node has a minimum sum of instance weight lower than min_child_weight in the tree partition step than the process of splitting the tree will stop growing.

7. subsample

The range of subsample is 0 to 1 and default value is 1. It controls the number of ratio observations to a tree. If the value is set to 0.5 means that XGboost would randomly sample half of the training data prior to growing trees and this will prevent overfitting. subsample will occur once in every boosting iteration.

8. colsampe_bytree

The range of colsample_bytree is 0 to 1 and default value is 1. It controls the subsample ratio of columns when constructing each tree.


##### Learning Task Parameters

Sets and evaluates the learning process of booster from the given data.

1. Objective
- `reg:squarederror` for regression with squared loss
- `binary:logistic`  for binary classification

2. eval_metric

Evaluation metrics for validaton data. The default is RMSE for regression case and error for classification case.

Next, we define the parameter will be used:

```{r}
params <- list(booster = "gbtree",
               objective = "binary:logistic",
               eta=0.1, 
               gamma=10, 
               max_depth=10, 
               min_child_weight=1, 
               subsample=1, 
               colsample_bytree=1)
```

One of the simplest way to see the training progress is to set the verbose option as `TRUE`.
```{r}
tic()
xgbcv <- xgb.cv( params = params, 
                 data = dtrain,
                 nrounds = 1000, 
                 showsd = T, 
                 nfold = 5,
                 stratified = T, 
                 print_every_n = 50, 
                 early_stopping_rounds = 20, 
                 maximize = F)
print(xgbcv)
toc()
```

```{r}
tic()
xgb1 <- xgb.train (params = params, 
                   data = dtrain, 
                   nrounds = xgbcv$best_iteration, 
                   watchlist = list(val=dtest,train=dtrain),
                   print_every_n = 100, 
                   early_stoping_rounds = 10, 
                   maximize = F , 
                   eval_metric = "error",
                   verbosity = 0)
toc()

xgbpred_prob <-predict(object = xgb1, newdata = dtest)
xgbpred <- ifelse (xgbpred_prob > 0.5,1,0)
```

In this section, we evaluate the performance of XGBoost model
```{r}
confusionMatrix(as.factor(xgbpred), as.factor(label_test))
```

let's check the variable importance from the model:
```{r}
var_imp <- xgb.importance(model = xgb1,
                          feature_names = dimnames(dtrain)[[2]])
var_imp %>% 
  mutate_if(is.numeric, round, digits = 2)

```

The function of `xgb.importance`displays the result importance values calculated with different importance metrics:

- The **gain** value means the percentage contribution of the feature for each tree in the model

- The **cover** value means the percentage represents the number of observations for each feature from all trees. From example, if we have 100 observations with 3 tree and each tree have 5, 8, and 10 observations for feature "A". The cover value will calculate 5+8+10 = 23 observations from all trees for each feature. In this case, the feature "A" has a 0.23 cover value.

- The **frequency** value means the percentage representing the number of times a feature will splits in the trees of the model. For example, feature "A" occurred in 3 splits, 2 splits, and 2 splits for each tree. So the value of frequency feature "A" is 3+2+2=7 splits divide with all total numbers splits for all features.

```{r}
xgb.ggplot.importance(var_imp,top_n = 10) + theme_minimal()
```

The graph shows the variable importance used the gain value by default and it also displays the cluster of features that have similar feature importances value. From 10 above features means their features have a significant impact on the result of prediction.

Next, we evaluate the perfomance model on the ROC curver
```{r}
xgb_result <- data.frame(class1 = xgbpred_prob, actual = as.factor(label_test))

auc_xgb <- roc_auc(data = xgb_result, truth = actual,class1) 

result <- rbind(auc_ada, auc_xgb) %>% 
          mutate(model = c("AdaBoost", "XGBoost")) %>% 
          select(model, everything())
result
```

The AUC results show that AdaBoost and XGBoost model have similar value 0.94 and 0.95. To obtain the AdaBoost model we need to run model for 60 minutes, while the XGBoost model only need ~60 seconds. We can say that XGBoost works better than AdaBoost for speed.

### Conclusion

In this article, we described the lesson on how to building and how AdaBoost and XGBoost model works. We can conclude several points:

- Both of two algorithms are built based on converting weak learners to a strong learner

- AdaBoost has only a few hyperparameters to improve the model but this model is easy to understand and to visualize

- The decision which algorithm will be used depends on our data set, for low noise data and timeliness of result is not the main concern, we can use AdaBoost model

- For complexity and high dimension data, XGBoost performs works better than Adaboost because XGBoost have system optimizations.


# Reference

[^1] : [XGBoost, a Top Machine Learning Method on Kaggle](https://www.kdnuggets.com/2017/10/xgboost-top-machine-learning-method-kaggle-explained.html)

[^2] : [XGBoost: The Excalibur for Everyone](https://towardsdatascience.com/xgboost-the-excalibur-for-everyone-8009bd015f1e)

[^3] : [Introduction to Boosted Trees](https://xgboost.readthedocs.io/en/latest/tutorials/model.html#)

[^4] : [Machine Learning Basics-Gradient Boosting & XGBoost](https://www.shirin-glander.de/2018/11/ml_basics_gbm/)

[^5] : [XGBoost Algorithm](source: https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)

[^6] : [Parameter Tuning in R](https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/)




